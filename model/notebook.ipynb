{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = './comments/'\n",
    "all_headlines = []\n",
    "for filename in os.listdir(curr_dir):\n",
    "    if 'ArticlesApril2017' in filename:\n",
    "        article_df = pd.read_csv(curr_dir + filename)\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Finding an Expansive View  of a Forgotten People in Niger', 'And Now,  the Dreaded Trump Curse', 'Venezuela’s Descent Into Dictatorship', 'Stain Permeates Basketball Blue Blood', 'Taking Things for Granted', 'The Caged Beast Awakens', 'An Ever-Unfolding Story', 'O’Reilly Thrives as Settlements Add Up', 'Mouse Infestation', 'Divide in G.O.P. Now Threatens Trump Tax Plan']\n"
     ]
    }
   ],
   "source": [
    "all_headlines = [line for line in all_headlines if line!= \"Unknown\"]\n",
    "print(all_headlines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(t for t in txt if t not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finding an expansive view  of a forgotten people in niger', 'and now  the dreaded trump curse', 'venezuelas descent into dictatorship', 'stain permeates basketball blue blood', 'taking things for granted', 'the caged beast awakens', 'an everunfolding story', 'oreilly thrives as settlements add up', 'mouse infestation', 'divide in gop now threatens trump tax plan', 'variety puzzle acrostic', 'they can hit a ball 400 feet but play catch thats tricky', 'in trump country shock at trump budget cuts', 'why is this hate different from all other hate']\n"
     ]
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "print(corpus[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'now', 'the', 'dreaded', 'trump', 'curse']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['venezuelas', 'descent', 'into', 'dictatorship']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(corpus), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_of_token(corpus):\n",
    "    input_sequnces = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer(line)\n",
    "        for i in range(0,len(token_list)):\n",
    "            n_gram_sequence = token_list[i:i+4]\n",
    "            if len(n_gram_sequence) > 3:\n",
    "                _n_gram_sequence  = vocab(n_gram_sequence)\n",
    "                input_sequnces.append(_n_gram_sequence)\n",
    "    return input_sequnces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 1104, 148, 1110]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['trump', 'descent', 'into', 'dictatorship'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_seq = get_sequence_of_token(corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[185, 18, 1219, 651],\n",
       " [18, 1219, 651, 5],\n",
       " [1219, 651, 5, 3],\n",
       " [651, 5, 3, 1289],\n",
       " [5, 3, 1289, 203]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2423"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = len(vocab)\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='float32')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_torch(arr,maxlen):\n",
    "    seq_len = len(arr)\n",
    "    padding = (maxlen-seq_len,0)\n",
    "    pad = torch.nn.ZeroPad2d(padding)\n",
    "    return pad(torch.tensor(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(inp_seq):\n",
    "    input_sequences = np.array(inp_seq)\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_len = round(len(inp_seq) * 0.2)\n",
    "test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, inp_seq):\n",
    "        self.input_sequences = np.array(inp_seq)\n",
    "         \n",
    "    def __getitem__(self, index):\n",
    "        fetures, label = self.input_sequences[index,:-1],self.input_sequences[index,-1]\n",
    "        # label = to_categorical(label, num_classes=total_words)\n",
    "        return fetures, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(inp_seq=inp_seq)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CreateDataset(inp_seq=random.sample(inp_seq,test_len))\n",
    "test_loader = DataLoader(test_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = iter(test_loader)\n",
    "feature, label = next(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[343,   4, 790]], dtype=torch.int32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18], dtype=torch.int32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2423, 10)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Embedding(total_words, 10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[185, 18, 1219, 651]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finding', 'an', 'expansive', 'view']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([185, 18, 1219, 651])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_on_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class RNN(nn.Module):\n",
    "    \n",
    "#     def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.3):\n",
    "#         super(RNN, self).__init__()\n",
    "             \n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.output_size = output_size\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.n_layers = n_layers\n",
    "        \n",
    "#         self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True,bidirectional=False)\n",
    "     \n",
    "#         self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    \n",
    "#     def forward(self, x, hidden1):\n",
    "#         x=x.long()\n",
    "        \n",
    "#         embeds = self.embedding(x)\n",
    "\n",
    "#         lstm_out, hidden1 = self.lstm1(embeds,hidden1)\n",
    "\n",
    "#         out  = lstm_out[:,-1,:]\n",
    "#         out = self.fc(out)\n",
    "\n",
    "#         # out = self.softmax(out)\n",
    "        \n",
    "\n",
    "#         return out, hidden1\n",
    "    \n",
    "#     def init_hidden(self,batch_size):\n",
    "#         hidden =  (torch.zeros(self.n_layers,batch_size,self.hidden_dim).to(device),\n",
    "#         torch.zeros(self.n_layers,batch_size,self.hidden_dim).to(device))\n",
    "#         return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden1):\n",
    "   \n",
    "#     h1 = tuple([each.data for each in hidden1])\n",
    "#     rnn.zero_grad()\n",
    "#     rnn.to(device)\n",
    "  \n",
    "#     inputs, targets = inp.to(device), target.to(device)\n",
    "    \n",
    "#     output, h1 = rnn(inputs, h1)\n",
    "    \n",
    "    \n",
    "  \n",
    "#     flag = torch.argmax(output).item() == torch.argmax(targets)\n",
    "#     # print(flag.item())\n",
    "#     # print(output.size())\n",
    "#     # print(.size())\n",
    "#     loss = criterion(output, targets.reshape(1,-1).softmax(dim=1))\n",
    "#     loss.backward()\n",
    "    \n",
    "#     nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "#     optimizer.step()\n",
    "\n",
    "#     return loss.item(), h1, flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "#     batch_losses = []\n",
    "    \n",
    "#     rnn.train()\n",
    "#     train_len = len(train_loader)\n",
    "#     print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "#     print(\"Train len: \" + str(train_len))\n",
    "\n",
    "#     for epoch_i in range(1, n_epochs + 1):\n",
    "#         acc = 0\n",
    "#         hidden1 = rnn.init_hidden(batch_size)\n",
    "#         for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "#             loss, hidden1, flag = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden1)  \n",
    "#             if flag:\n",
    "#                 acc += 1        \n",
    "#             batch_losses.append(loss)\n",
    "\n",
    "       \n",
    "#         print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "#                     epoch_i, n_epochs, np.average(batch_losses)))\n",
    "#         batch_losses = []\n",
    "#         print(acc)\n",
    "#         print(train_len)\n",
    "#         print(f'Epoch: {epoch_i}, accuracy: {(acc/train_len)*100}')\n",
    "#     return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_length = 4  # of words in a sequence\n",
    "# # Batch Size\n",
    "# batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 50\n",
    "# # Learning Rate\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# # Model parameters\n",
    "# # Vocab size\n",
    "# vocab_size = len(vocab)\n",
    "# # Output size\n",
    "# output_size = vocab_size\n",
    "# # Embedding Dimension\n",
    "# embedding_dim = 1024\n",
    "# # Hidden Dimension\n",
    "# hidden_dim = 1024\n",
    "# # Number of RNN Layers\n",
    "# n_layers = 2\n",
    "\n",
    "# show_every_n_batches = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "# print(rnn)\n",
    "# if train_on_gpu:\n",
    "#     rnn.to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # training the model\n",
    "# trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, \n",
    "                tie_weights):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        if tie_weights:\n",
    "            assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
    "            self.embedding.weight = self.fc.weight\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.embedding(src)\n",
    "        output, hidden = self.lstm(embedding, hidden)          \n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128            \n",
    "hidden_dim = 128               \n",
    "num_layers = 3             \n",
    "dropout_rate = 0.15          \n",
    "tie_weights = True                  \n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2423"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3193"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 708,855 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "   \n",
    "\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        \n",
    "        src, target = inputs, labels\n",
    "        src, target = src.to(device), target.to(device)\n",
    "    \n",
    "      \n",
    "        prediction, hidden = model(src, hidden) \n",
    "        prediction  = prediction[:,-1,:]              \n",
    "       \n",
    "        target = target.reshape(-1).to(torch.int64)\n",
    "       \n",
    "        correct += (target == torch.argmax(prediction,dim=1)).sum()\n",
    "      \n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(correct)\n",
    "    print(f'Accuracy: {(100 * (correct /len(train_loader)))}')\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    correct =0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_i, (inputs, labels) in enumerate(test_loader, 1):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = inputs, labels\n",
    "            src, target = src.to(device), target.to(device)\n",
    "         \n",
    "            prediction, hidden = model(src, hidden) \n",
    "            prediction  = prediction[:,-1,:]              \n",
    "            \n",
    "            target = target.reshape(-1).to(torch.int64)\n",
    "\n",
    "            correct += (target == torch.argmax(prediction,dim=1)).sum()\n",
    "           \n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    print(correct)\n",
    "    print(f'Accuracy: {(100 * (correct /len(test_loader)))}')\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(93, device='cuda:0')\n",
      "Accuracy: 2.91262149810791\n",
      "\tTrain Perplexity: inf\n",
      "tensor(113, device='cuda:0')\n",
      "Accuracy: 3.5389914512634277\n",
      "\tTrain Perplexity: inf\n",
      "tensor(119, device='cuda:0')\n",
      "Accuracy: 3.726902484893799\n",
      "\tTrain Perplexity: inf\n",
      "tensor(117, device='cuda:0')\n",
      "Accuracy: 3.6642656326293945\n",
      "\tTrain Perplexity: inf\n",
      "tensor(132, device='cuda:0')\n",
      "Accuracy: 4.134043216705322\n",
      "\tTrain Perplexity: inf\n",
      "tensor(162, device='cuda:0')\n",
      "Accuracy: 5.073598384857178\n",
      "\tTrain Perplexity: inf\n",
      "tensor(158, device='cuda:0')\n",
      "Accuracy: 4.948324680328369\n",
      "\tTrain Perplexity: inf\n",
      "tensor(177, device='cuda:0')\n",
      "Accuracy: 5.5433759689331055\n",
      "\tTrain Perplexity: inf\n",
      "tensor(205, device='cuda:0')\n",
      "Accuracy: 6.420294284820557\n",
      "\tTrain Perplexity: inf\n",
      "tensor(227, device='cuda:0')\n",
      "Accuracy: 7.109301567077637\n",
      "\tTrain Perplexity: inf\n",
      "tensor(257, device='cuda:0')\n",
      "Accuracy: 8.048856735229492\n",
      "\tTrain Perplexity: inf\n",
      "tensor(276, device='cuda:0')\n",
      "Accuracy: 8.643908500671387\n",
      "\tTrain Perplexity: inf\n",
      "tensor(301, device='cuda:0')\n",
      "Accuracy: 9.426871299743652\n",
      "\tTrain Perplexity: inf\n",
      "tensor(343, device='cuda:0')\n",
      "Accuracy: 10.74224853515625\n",
      "\tTrain Perplexity: inf\n",
      "tensor(390, device='cuda:0')\n",
      "Accuracy: 12.214219093322754\n",
      "\tTrain Perplexity: inf\n",
      "tensor(417, device='cuda:0')\n",
      "Accuracy: 13.059818267822266\n",
      "\tTrain Perplexity: inf\n",
      "tensor(446, device='cuda:0')\n",
      "Accuracy: 13.96805477142334\n",
      "\tTrain Perplexity: inf\n",
      "tensor(489, device='cuda:0')\n",
      "Accuracy: 15.314751625061035\n",
      "\tTrain Perplexity: inf\n",
      "tensor(517, device='cuda:0')\n",
      "Accuracy: 16.191669464111328\n",
      "\tTrain Perplexity: inf\n",
      "tensor(572, device='cuda:0')\n",
      "Accuracy: 17.914188385009766\n",
      "\tTrain Perplexity: inf\n",
      "tensor(571, device='cuda:0')\n",
      "Accuracy: 17.88286781311035\n",
      "\tTrain Perplexity: inf\n",
      "tensor(629, device='cuda:0')\n",
      "Accuracy: 19.699342727661133\n",
      "\tTrain Perplexity: inf\n",
      "tensor(652, device='cuda:0')\n",
      "Accuracy: 20.419668197631836\n",
      "\tTrain Perplexity: inf\n",
      "tensor(712, device='cuda:0')\n",
      "Accuracy: 22.298778533935547\n",
      "\tTrain Perplexity: inf\n",
      "tensor(740, device='cuda:0')\n",
      "Accuracy: 23.175697326660156\n",
      "\tTrain Perplexity: inf\n",
      "tensor(773, device='cuda:0')\n",
      "Accuracy: 24.20920753479004\n",
      "\tTrain Perplexity: inf\n",
      "tensor(791, device='cuda:0')\n",
      "Accuracy: 24.772939682006836\n",
      "\tTrain Perplexity: inf\n",
      "tensor(822, device='cuda:0')\n",
      "Accuracy: 25.74381446838379\n",
      "\tTrain Perplexity: inf\n",
      "tensor(894, device='cuda:0')\n",
      "Accuracy: 27.998748779296875\n",
      "\tTrain Perplexity: inf\n",
      "tensor(930, device='cuda:0')\n",
      "Accuracy: 29.1262149810791\n",
      "\tTrain Perplexity: inf\n",
      "tensor(965, device='cuda:0')\n",
      "Accuracy: 30.222362518310547\n",
      "\tTrain Perplexity: inf\n",
      "tensor(997, device='cuda:0')\n",
      "Accuracy: 31.22455406188965\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1038, device='cuda:0')\n",
      "Accuracy: 32.50861358642578\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1082, device='cuda:0')\n",
      "Accuracy: 33.886627197265625\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1102, device='cuda:0')\n",
      "Accuracy: 34.512996673583984\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1115, device='cuda:0')\n",
      "Accuracy: 34.92013931274414\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1177, device='cuda:0')\n",
      "Accuracy: 36.86188507080078\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1181, device='cuda:0')\n",
      "Accuracy: 36.987159729003906\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1209, device='cuda:0')\n",
      "Accuracy: 37.864078521728516\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1255, device='cuda:0')\n",
      "Accuracy: 39.30472946166992\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1324, device='cuda:0')\n",
      "Accuracy: 41.46570587158203\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1291, device='cuda:0')\n",
      "Accuracy: 40.43219757080078\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1343, device='cuda:0')\n",
      "Accuracy: 42.060760498046875\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1363, device='cuda:0')\n",
      "Accuracy: 42.687129974365234\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1393, device='cuda:0')\n",
      "Accuracy: 43.62668228149414\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1411, device='cuda:0')\n",
      "Accuracy: 44.19041442871094\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1463, device='cuda:0')\n",
      "Accuracy: 45.81897735595703\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1437, device='cuda:0')\n",
      "Accuracy: 45.00469970703125\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1519, device='cuda:0')\n",
      "Accuracy: 47.57281494140625\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1572, device='cuda:0')\n",
      "Accuracy: 49.232696533203125\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1534, device='cuda:0')\n",
      "Accuracy: 48.04259490966797\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1531, device='cuda:0')\n",
      "Accuracy: 47.948638916015625\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1579, device='cuda:0')\n",
      "Accuracy: 49.451927185058594\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1585, device='cuda:0')\n",
      "Accuracy: 49.63983917236328\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1635, device='cuda:0')\n",
      "Accuracy: 51.20576477050781\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1647, device='cuda:0')\n",
      "Accuracy: 51.58158493041992\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1681, device='cuda:0')\n",
      "Accuracy: 52.64641571044922\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1699, device='cuda:0')\n",
      "Accuracy: 53.21014404296875\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1718, device='cuda:0')\n",
      "Accuracy: 53.80520248413086\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1731, device='cuda:0')\n",
      "Accuracy: 54.212337493896484\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1756, device='cuda:0')\n",
      "Accuracy: 54.995304107666016\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1779, device='cuda:0')\n",
      "Accuracy: 55.71562576293945\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1772, device='cuda:0')\n",
      "Accuracy: 55.49639892578125\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1815, device='cuda:0')\n",
      "Accuracy: 56.84309768676758\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1806, device='cuda:0')\n",
      "Accuracy: 56.56122589111328\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1821, device='cuda:0')\n",
      "Accuracy: 57.031005859375\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1867, device='cuda:0')\n",
      "Accuracy: 58.471656799316406\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1841, device='cuda:0')\n",
      "Accuracy: 57.657379150390625\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1843, device='cuda:0')\n",
      "Accuracy: 57.72001266479492\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1902, device='cuda:0')\n",
      "Accuracy: 59.56780242919922\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1901, device='cuda:0')\n",
      "Accuracy: 59.5364875793457\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1924, device='cuda:0')\n",
      "Accuracy: 60.256813049316406\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1956, device='cuda:0')\n",
      "Accuracy: 61.25900650024414\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1915, device='cuda:0')\n",
      "Accuracy: 59.974945068359375\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1942, device='cuda:0')\n",
      "Accuracy: 60.8205451965332\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1959, device='cuda:0')\n",
      "Accuracy: 61.352962493896484\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1967, device='cuda:0')\n",
      "Accuracy: 61.603511810302734\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2013, device='cuda:0')\n",
      "Accuracy: 63.044158935546875\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2005, device='cuda:0')\n",
      "Accuracy: 62.79361343383789\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1977, device='cuda:0')\n",
      "Accuracy: 61.916690826416016\n",
      "\tTrain Perplexity: inf\n",
      "tensor(1980, device='cuda:0')\n",
      "Accuracy: 62.010650634765625\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2030, device='cuda:0')\n",
      "Accuracy: 63.57657241821289\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2058, device='cuda:0')\n",
      "Accuracy: 64.4534912109375\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2051, device='cuda:0')\n",
      "Accuracy: 64.23426055908203\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2085, device='cuda:0')\n",
      "Accuracy: 65.2990951538086\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2062, device='cuda:0')\n",
      "Accuracy: 64.57876586914062\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2074, device='cuda:0')\n",
      "Accuracy: 64.95458984375\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2113, device='cuda:0')\n",
      "Accuracy: 66.17601013183594\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2100, device='cuda:0')\n",
      "Accuracy: 65.76886749267578\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2072, device='cuda:0')\n",
      "Accuracy: 64.89195251464844\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2109, device='cuda:0')\n",
      "Accuracy: 66.05073547363281\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2106, device='cuda:0')\n",
      "Accuracy: 65.956787109375\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2141, device='cuda:0')\n",
      "Accuracy: 67.05293273925781\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2128, device='cuda:0')\n",
      "Accuracy: 66.64579010009766\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2168, device='cuda:0')\n",
      "Accuracy: 67.89852905273438\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2141, device='cuda:0')\n",
      "Accuracy: 67.05293273925781\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2129, device='cuda:0')\n",
      "Accuracy: 66.67710876464844\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2160, device='cuda:0')\n",
      "Accuracy: 67.64797973632812\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2191, device='cuda:0')\n",
      "Accuracy: 68.61885070800781\n",
      "\tTrain Perplexity: inf\n",
      "tensor(2188, device='cuda:0')\n",
      "Accuracy: 68.52489471435547\n",
      "\tTrain Perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 100\n",
    "seq_len = 3\n",
    "clip = 0.25\n",
    "saved = False\n",
    "\n",
    "# lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=20)\n",
    "\n",
    "if saved:\n",
    "    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "    test_loss = evaluate(model, criterion, device)\n",
    "    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\n",
    "else:\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, optimizer, criterion, clip, device)\n",
    "        # valid_loss = evaluate_new(model, None, criterion, batch_size, \n",
    "        #             seq_len, device)\n",
    "        \n",
    "        # lr_scheduler.step(valid_loss)\n",
    "\n",
    "        # if valid_loss < best_valid_loss:\n",
    "        #     best_valid_loss = valid_loss\n",
    "        #     torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "        try:\n",
    "            print(f'\\tTrain Perplexity: {math.exp(train_loss):.7f}')\n",
    "            # print(f'\\tValid Perplexity: {math.exp(valid_loss):.7f}')\n",
    "        except OverflowError:\n",
    "            print(f'\\tTrain Perplexity: {math.inf}')\n",
    "            # print(f'\\tValid Perplexity: {math.inf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(562, device='cuda:0')\n",
      "Accuracy: 87.94992065429688\n"
     ]
    }
   ],
   "source": [
    "valid_loss = evaluate(model, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(str_original):\n",
    "    str = tokenizer(clean_text(str_original))\n",
    "    print(str)\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    with torch.no_grad():  \n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        src = torch.tensor([vocab(str)]).to(torch.int32).to(device)\n",
    "        prediction, hidden = model(src, hidden) \n",
    "        prediction  = prediction[:,-1,:]              \n",
    "        print(f'{str_original} --> {vocab.lookup_tokens([torch.argmax(prediction,dim=1).item()])[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['search', 'of', 'king']\n",
      "search of king --> solomons\n"
     ]
    }
   ],
   "source": [
    "predict_next_word('search of king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'national', 'civics']\n",
      "A National Civics --> exam\n"
     ]
    }
   ],
   "source": [
    "predict_next_word('A National Civics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['picks', 'for', 'education']\n",
      "Picks for Education --> dept\n"
     ]
    }
   ],
   "source": [
    "predict_next_word('Picks for Education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['center', 'of', 'the']\n",
      " Center of the --> universe\n"
     ]
    }
   ],
   "source": [
    "predict_next_word(' Center of the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['supreme', 'court', 'as']\n",
      "Supreme Court as --> tragic\n"
     ]
    }
   ],
   "source": [
    "predict_next_word('Supreme Court as')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

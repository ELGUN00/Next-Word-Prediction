{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = './comments/'\n",
    "all_headlines = []\n",
    "for filename in os.listdir(curr_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(curr_dir + filename)\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Finding an Expansive View  of a Forgotten People in Niger', 'And Now,  the Dreaded Trump Curse', 'Venezuela’s Descent Into Dictatorship', 'Stain Permeates Basketball Blue Blood', 'Taking Things for Granted', 'The Caged Beast Awakens', 'An Ever-Unfolding Story', 'O’Reilly Thrives as Settlements Add Up', 'Mouse Infestation', 'Divide in G.O.P. Now Threatens Trump Tax Plan']\n"
     ]
    }
   ],
   "source": [
    "all_headlines = [line for line in all_headlines if line!= \"Unknown\"]\n",
    "print(all_headlines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(t for t in txt if t not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finding an expansive view  of a forgotten people in niger', 'and now  the dreaded trump curse', 'venezuelas descent into dictatorship', 'stain permeates basketball blue blood', 'taking things for granted', 'the caged beast awakens', 'an everunfolding story', 'oreilly thrives as settlements add up', 'mouse infestation', 'divide in gop now threatens trump tax plan', 'variety puzzle acrostic', 'they can hit a ball 400 feet but play catch thats tricky', 'in trump country shock at trump budget cuts', 'why is this hate different from all other hate']\n"
     ]
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "print(corpus[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'now', 'the', 'dreaded', 'trump', 'curse']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(corpus), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_of_token(corpus):\n",
    "    input_sequnces = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer(line)\n",
    "        for i in range(1,len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            _n_gram_sequence  = vocab(n_gram_sequence)\n",
    "            input_sequnces.append(_n_gram_sequence)\n",
    "    return input_sequnces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_seq = get_sequence_of_token(corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2423"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = len(vocab)\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='float32')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_torch(arr,maxlen):\n",
    "    seq_len = len(arr)\n",
    "    padding = (maxlen-seq_len,0)\n",
    "    pad = torch.nn.ZeroPad2d(padding)\n",
    "    return pad(torch.tensor(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(inp_seq):\n",
    "    max_sequence_len = max([len(x) for x in inp_seq])\n",
    "    res_1 = [pad_sequences_torch(i,max_sequence_len) for i in inp_seq]\n",
    "    input_sequences = np.array(res_1)\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4806"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, inp_seq):\n",
    "        self.inp_seq = inp_seq\n",
    "        max_sequence_len = max([len(x) for x in self.inp_seq])\n",
    "        res_1 = [pad_sequences_torch(i,max_sequence_len) for i in self.inp_seq]\n",
    "        self.input_sequences = np.array(res_1)\n",
    "         \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        fetures, label = self.input_sequences[index,:-1],self.input_sequences[index,-1]\n",
    "        label = to_categorical(label, num_classes=total_words)\n",
    "        return fetures, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inp_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(inp_seq=inp_seq)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = iter(train_loader)\n",
    "feature, label = next(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  59, 396,   7])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, label, max_seq = generate_padded_sequences(inp_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,  185],\n",
       "       [   0,    0,    0, ...,    0,  185,   18],\n",
       "       [   0,    0,    0, ...,  185,   18, 1219],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  101, 2180,   57],\n",
       "       [   0,    0,    0, ..., 2180,   57,  347],\n",
       "       [   0,    0,    0, ...,   57,  347,   95]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4806, 2423)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2423, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Embedding(total_words, 10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[185, 18]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "      \n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "     \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x=x.long()\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1]\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weights = next(self.parameters()).data\n",
    "        if(train_on_gpu):\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), \n",
    "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "   \n",
    "    h = tuple([each.data for each in hidden])\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "  \n",
    "    inputs, targets = inp.to(device), target.to(device)\n",
    "    \n",
    "    output, h = rnn(inputs, h)\n",
    "    \n",
    "    loss = criterion(output, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 18  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab)\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 200\n",
    "# Hidden Dimension\n",
    "hidden_dim = 250\n",
    "# Number of RNN Layers\n",
    "n_layers = 4\n",
    "\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(2423, 200)\n",
      "  (lstm): LSTM(200, 250, num_layers=4, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=250, out_features=2423, bias=True)\n",
      ")\n",
      "Training for 10 epoch(s)...\n",
      "Epoch:    1/10    Loss: 0.01564548803237267\n",
      "\n",
      "Epoch:    1/10    Loss: 0.0036303741591982544\n",
      "\n",
      "Epoch:    1/10    Loss: 0.003786800573579967\n",
      "\n",
      "Epoch:    1/10    Loss: 0.0038206388072576374\n",
      "\n",
      "Epoch:    1/10    Loss: 0.003827467465540394\n",
      "\n",
      "Epoch:    1/10    Loss: 0.0037638499792665243\n",
      "\n",
      "Epoch:    1/10    Loss: 0.003715909888036549\n",
      "\n",
      "Epoch:    1/10    Loss: 0.003695638798875734\n",
      "\n",
      "Epoch:    1/10    Loss: 0.003617398198926821\n",
      "\n",
      "Epoch:    2/10    Loss: 0.0034660280694568035\n",
      "\n",
      "Epoch:    2/10    Loss: 0.003443441507406533\n",
      "\n",
      "Epoch:    2/10    Loss: 0.0034820756090339273\n",
      "\n",
      "Epoch:    2/10    Loss: 0.0034912491112481805\n",
      "\n",
      "Epoch:    2/10    Loss: 0.0035138779666740445\n",
      "\n",
      "Epoch:    2/10    Loss: 0.0033886819726321846\n",
      "\n",
      "Epoch:    2/10    Loss: 0.0034240900070872157\n",
      "\n",
      "Epoch:    2/10    Loss: 0.003537353924708441\n",
      "\n",
      "Epoch:    2/10    Loss: 0.003421511522028595\n",
      "\n",
      "Epoch:    3/10    Loss: 0.0033371259885026654\n",
      "\n",
      "Epoch:    3/10    Loss: 0.0033525345702655612\n",
      "\n",
      "Epoch:    3/10    Loss: 0.003441084415651858\n",
      "\n",
      "Epoch:    3/10    Loss: 0.0033933417457155885\n",
      "\n",
      "Epoch:    3/10    Loss: 0.003409995095571503\n",
      "\n",
      "Epoch:    3/10    Loss: 0.003470143679296598\n",
      "\n",
      "Epoch:    3/10    Loss: 0.003435709395678714\n",
      "\n",
      "Epoch:    3/10    Loss: 0.0033850587513297795\n",
      "\n",
      "Epoch:    3/10    Loss: 0.0034574116778094324\n",
      "\n",
      "Epoch:    4/10    Loss: 0.0033690910094542423\n",
      "\n",
      "Epoch:    4/10    Loss: 0.0033651371963787824\n",
      "\n",
      "Epoch:    4/10    Loss: 0.00330450943717733\n",
      "\n",
      "Epoch:    4/10    Loss: 0.0033781175427138807\n",
      "\n",
      "Epoch:    4/10    Loss: 0.0033698902002070098\n",
      "\n",
      "Epoch:    4/10    Loss: 0.003389365762239322\n",
      "\n",
      "Epoch:    4/10    Loss: 0.0033895977267529814\n",
      "\n",
      "Epoch:    4/10    Loss: 0.0033915333945769817\n",
      "\n",
      "Epoch:    4/10    Loss: 0.0033987267496995628\n",
      "\n",
      "Epoch:    5/10    Loss: 0.0033233910162064244\n",
      "\n",
      "Epoch:    5/10    Loss: 0.0033562210362870245\n",
      "\n",
      "Epoch:    5/10    Loss: 0.003228891177335754\n",
      "\n",
      "Epoch:    5/10    Loss: 0.003382260159123689\n",
      "\n",
      "Epoch:    5/10    Loss: 0.0033116166435647756\n",
      "\n",
      "Epoch:    5/10    Loss: 0.003379013965371996\n",
      "\n",
      "Epoch:    5/10    Loss: 0.0033433414762839677\n",
      "\n",
      "Epoch:    5/10    Loss: 0.00339791342522949\n",
      "\n",
      "Epoch:    5/10    Loss: 0.0034264783493708818\n",
      "\n",
      "Epoch:    6/10    Loss: 0.003301259210256966\n",
      "\n",
      "Epoch:    6/10    Loss: 0.0032660145361442117\n",
      "\n",
      "Epoch:    6/10    Loss: 0.0032920651680324224\n",
      "\n",
      "Epoch:    6/10    Loss: 0.0033254408959764985\n",
      "\n",
      "Epoch:    6/10    Loss: 0.003329549947520718\n",
      "\n",
      "Epoch:    6/10    Loss: 0.0033598528371658175\n",
      "\n",
      "Epoch:    6/10    Loss: 0.00334844111581333\n",
      "\n",
      "Epoch:    6/10    Loss: 0.0033155656091403214\n",
      "\n",
      "Epoch:    6/10    Loss: 0.003411902182037011\n",
      "\n",
      "Epoch:    7/10    Loss: 0.003321040221247314\n",
      "\n",
      "Epoch:    7/10    Loss: 0.0032982443044893444\n",
      "\n",
      "Epoch:    7/10    Loss: 0.003240221794229001\n",
      "\n",
      "Epoch:    7/10    Loss: 0.0032317291477229446\n",
      "\n",
      "Epoch:    7/10    Loss: 0.0032970997309312226\n",
      "\n",
      "Epoch:    7/10    Loss: 0.0032333880679216234\n",
      "\n",
      "Epoch:    7/10    Loss: 0.0033575146060902624\n",
      "\n",
      "Epoch:    7/10    Loss: 0.003373919497942552\n",
      "\n",
      "Epoch:    7/10    Loss: 0.0034059434530790897\n",
      "\n",
      "Epoch:    8/10    Loss: 0.0032646974523276757\n",
      "\n",
      "Epoch:    8/10    Loss: 0.003245079811895266\n",
      "\n",
      "Epoch:    8/10    Loss: 0.0032729866430163384\n",
      "\n",
      "Epoch:    8/10    Loss: 0.003234117549844086\n",
      "\n",
      "Epoch:    8/10    Loss: 0.003279697749763727\n",
      "\n",
      "Epoch:    8/10    Loss: 0.003333439960377291\n",
      "\n",
      "Epoch:    8/10    Loss: 0.003314044099766761\n",
      "\n",
      "Epoch:    8/10    Loss: 0.0033542007887735962\n",
      "\n",
      "Epoch:    8/10    Loss: 0.0034090151970740407\n",
      "\n",
      "Epoch:    9/10    Loss: 0.0032222381226485017\n",
      "\n",
      "Epoch:    9/10    Loss: 0.0032647332074120642\n",
      "\n",
      "Epoch:    9/10    Loss: 0.0032746131103485823\n",
      "\n",
      "Epoch:    9/10    Loss: 0.003210378929739818\n",
      "\n",
      "Epoch:    9/10    Loss: 0.003283856202149764\n",
      "\n",
      "Epoch:    9/10    Loss: 0.003312360155628994\n",
      "\n",
      "Epoch:    9/10    Loss: 0.0033593104253523054\n",
      "\n",
      "Epoch:    9/10    Loss: 0.0033274970746133476\n",
      "\n",
      "Epoch:    9/10    Loss: 0.003407954370835796\n",
      "\n",
      "Epoch:   10/10    Loss: 0.003280378989765545\n",
      "\n",
      "Epoch:   10/10    Loss: 0.0032261006515473126\n",
      "\n",
      "Epoch:   10/10    Loss: 0.0032485713779460638\n",
      "\n",
      "Epoch:   10/10    Loss: 0.003211296113440767\n",
      "\n",
      "Epoch:   10/10    Loss: 0.003275342298671603\n",
      "\n",
      "Epoch:   10/10    Loss: 0.0032468259073793887\n",
      "\n",
      "Epoch:   10/10    Loss: 0.0033624802734702826\n",
      "\n",
      "Epoch:   10/10    Loss: 0.0033401084502693267\n",
      "\n",
      "Epoch:   10/10    Loss: 0.003316705733537674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "print(rnn)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = rnn.init_hidden(batch_size)\n",
    "h = tuple([each.data for each in hidden])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2, 12, 31])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = tokenizer('the new york')\n",
    "text  = vocab(token_list)\n",
    "text = pad_sequences_torch(text,18)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rnn(text.reshape(1,-1),h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(242)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output[-1][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cant']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([242])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

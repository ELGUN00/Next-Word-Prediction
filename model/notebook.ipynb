{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"corpus.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Next Word Prediction is also called Language Modeling. It is the task of predicting what word comes next. It is one of the fundamental tasks of NLP and has many applications. You might be using it daily when you write texts or emails without realizing it.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "corpus = corpus.lower()\n",
    "clean_corpus = re.sub('[^a-z0-9]+',' ', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'next word prediction is also called language modeling it is the task of predicting what word comes next it is one of the fundamental tasks of nlp and has many applications you might be using it daily when you write texts or emails without realizing it '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#required libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['next',\n",
       " 'word',\n",
       " 'prediction',\n",
       " 'is',\n",
       " 'also',\n",
       " 'called',\n",
       " 'language',\n",
       " 'modeling',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'task',\n",
       " 'of',\n",
       " 'predicting',\n",
       " 'what',\n",
       " 'word',\n",
       " 'comes',\n",
       " 'next',\n",
       " 'it',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'tasks',\n",
       " 'of',\n",
       " 'nlp',\n",
       " 'and',\n",
       " 'has',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'you',\n",
       " 'might',\n",
       " 'be',\n",
       " 'using',\n",
       " 'it',\n",
       " 'daily',\n",
       " 'when',\n",
       " 'you',\n",
       " 'write',\n",
       " 'texts',\n",
       " 'or',\n",
       " 'emails',\n",
       " 'without',\n",
       " 'realizing',\n",
       " 'it']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(clean_corpus)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>image</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.png</td>\n",
       "      <td>850</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.png</td>\n",
       "      <td>1100</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>A Grammar of Graphics for Python</td>\n",
       "      <td>3.png</td>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://towardsdatascience.com/databricks-how-...</td>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "      <td>When I work on Python projects dealing…</td>\n",
       "      <td>4.jpeg</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>One example of building neural…</td>\n",
       "      <td>5.jpeg</td>\n",
       "      <td>211</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>6504</td>\n",
       "      <td>https://medium.com/better-marketing/we-vs-i-ho...</td>\n",
       "      <td>“We” vs “I” — How Should You Talk About Yourse...</td>\n",
       "      <td>Basic copywriting choices with a big…</td>\n",
       "      <td>6504.jpg</td>\n",
       "      <td>661</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6504</th>\n",
       "      <td>6505</td>\n",
       "      <td>https://medium.com/better-marketing/how-donald...</td>\n",
       "      <td>How Donald Trump Markets Himself</td>\n",
       "      <td>Lessons from who might be the most popular bra...</td>\n",
       "      <td>6505.jpeg</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>6506</td>\n",
       "      <td>https://medium.com/better-marketing/content-an...</td>\n",
       "      <td>Content and Marketing Beyond Mass Consumption</td>\n",
       "      <td>How to acquire customers without wasting money...</td>\n",
       "      <td>6506.jpg</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>6507</td>\n",
       "      <td>https://medium.com/better-marketing/5-question...</td>\n",
       "      <td>5 Questions All Copywriters Should Ask Clients...</td>\n",
       "      <td>Save time and effort by…</td>\n",
       "      <td>6507.jpg</td>\n",
       "      <td>253</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>6508</td>\n",
       "      <td>https://medium.com/better-marketing/how-to-wri...</td>\n",
       "      <td>How To Write a Good Business Blog Post</td>\n",
       "      <td>An A-to-Z guide for non-writers</td>\n",
       "      <td>6508.jpg</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Better Marketing</td>\n",
       "      <td>2019-12-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6508 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                url  \\\n",
       "0        1  https://towardsdatascience.com/a-beginners-gui...   \n",
       "1        2  https://towardsdatascience.com/hands-on-graph-...   \n",
       "2        3  https://towardsdatascience.com/how-to-use-ggpl...   \n",
       "3        4  https://towardsdatascience.com/databricks-how-...   \n",
       "4        5  https://towardsdatascience.com/a-step-by-step-...   \n",
       "...    ...                                                ...   \n",
       "6503  6504  https://medium.com/better-marketing/we-vs-i-ho...   \n",
       "6504  6505  https://medium.com/better-marketing/how-donald...   \n",
       "6505  6506  https://medium.com/better-marketing/content-an...   \n",
       "6506  6507  https://medium.com/better-marketing/5-question...   \n",
       "6507  6508  https://medium.com/better-marketing/how-to-wri...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1     Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                          How to Use ggplot2 in Python   \n",
       "3     Databricks: How to Save Files in CSV on Your L...   \n",
       "4     A Step-by-Step Implementation of Gradient Desc...   \n",
       "...                                                 ...   \n",
       "6503  “We” vs “I” — How Should You Talk About Yourse...   \n",
       "6504                   How Donald Trump Markets Himself   \n",
       "6505      Content and Marketing Beyond Mass Consumption   \n",
       "6506  5 Questions All Copywriters Should Ask Clients...   \n",
       "6507             How To Write a Good Business Blog Post   \n",
       "\n",
       "                                               subtitle      image  claps  \\\n",
       "0                                                   NaN      1.png    850   \n",
       "1                                                   NaN      2.png   1100   \n",
       "2                      A Grammar of Graphics for Python      3.png    767   \n",
       "3               When I work on Python projects dealing…     4.jpeg    354   \n",
       "4                       One example of building neural…     5.jpeg    211   \n",
       "...                                                 ...        ...    ...   \n",
       "6503              Basic copywriting choices with a big…   6504.jpg    661   \n",
       "6504  Lessons from who might be the most popular bra...  6505.jpeg    189   \n",
       "6505  How to acquire customers without wasting money...   6506.jpg    207   \n",
       "6506                           Save time and effort by…   6507.jpg    253   \n",
       "6507                    An A-to-Z guide for non-writers   6508.jpg    147   \n",
       "\n",
       "     responses  reading_time           publication        date  \n",
       "0            8             8  Towards Data Science  2019-05-30  \n",
       "1           11             9  Towards Data Science  2019-05-30  \n",
       "2            1             5  Towards Data Science  2019-05-30  \n",
       "3            0             4  Towards Data Science  2019-05-30  \n",
       "4            3             4  Towards Data Science  2019-05-30  \n",
       "...        ...           ...                   ...         ...  \n",
       "6503         6             6      Better Marketing  2019-12-05  \n",
       "6504         1             5      Better Marketing  2019-12-05  \n",
       "6505         1             8      Better Marketing  2019-12-05  \n",
       "6506         2             5      Better Marketing  2019-12-05  \n",
       "6507         0             9      Better Marketing  2019-12-05  \n",
       "\n",
       "[6508 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('medium_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Databricks: How to Save Files in CSV on Your Local\\xa0Computer'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\n",
    "df['title'] = df['title'].apply(lambda x: x.replace('\\u200a',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Databricks: How to Save Files in CSV on Your Local Computer'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['databricks',\n",
       " 'how',\n",
       " 'to',\n",
       " 'save',\n",
       " 'files',\n",
       " 'in',\n",
       " 'csv',\n",
       " 'on',\n",
       " 'your',\n",
       " 'local',\n",
       " 'computer']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Databricks: How to Save Files in CSV on Your Local Computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['title']), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['write', 'a', 'good']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab([\"pytorch\",'hello','and'])\n",
    "vocab.lookup_tokens([ 68,   4,  89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['write', 'a', 'good']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([ 68,   4,  89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def made_tokens(train_len,tokens,arr):\n",
    "    for i in range(train_len,len(tokens)+1):\n",
    "        seq = tokens[i-train_len:i]\n",
    "        arr.append(vocab(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in df['title']:\n",
    "  i=i.lower()\n",
    "  tokens = tokenizer(i)\n",
    "  made_tokens(train_len=3,tokens=tokens,arr=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 659, 72],\n",
       " [659, 72, 2],\n",
       " [72, 2, 468],\n",
       " [2, 468, 1585],\n",
       " [468, 1585, 14],\n",
       " [1585, 14, 2876],\n",
       " [14, 2876, 8754],\n",
       " [2876, 8754, 104],\n",
       " [5727, 747, 85],\n",
       " [747, 85, 107],\n",
       " [85, 107, 14],\n",
       " [107, 14, 349],\n",
       " [14, 349, 54],\n",
       " [349, 54, 349],\n",
       " [54, 349, 2061],\n",
       " [7, 2, 69],\n",
       " [2, 69, 5608],\n",
       " [69, 5608, 8],\n",
       " [5608, 8, 43],\n",
       " [4893, 7, 2],\n",
       " [7, 2, 266],\n",
       " [2, 266, 1598],\n",
       " [266, 1598, 8],\n",
       " [1598, 8, 2689],\n",
       " [8, 2689, 24],\n",
       " [2689, 24, 9],\n",
       " [24, 9, 991],\n",
       " [9, 991, 268],\n",
       " [4, 1024, 485],\n",
       " [1024, 485, 5],\n",
       " [485, 5, 2068],\n",
       " [5, 2068, 1302],\n",
       " [2068, 1302, 6],\n",
       " [1302, 6, 1509],\n",
       " [27, 199, 124],\n",
       " [199, 124, 2],\n",
       " [124, 2, 461],\n",
       " [2, 461, 11],\n",
       " [461, 11, 18],\n",
       " [11, 18, 366],\n",
       " [1634, 296, 8646],\n",
       " [124, 2, 6209],\n",
       " [2, 6209, 755],\n",
       " [6209, 755, 2821],\n",
       " [755, 2821, 769],\n",
       " [2821, 769, 330],\n",
       " [398, 226, 1892],\n",
       " [226, 1892, 15],\n",
       " [1892, 15, 3],\n",
       " [15, 3, 78],\n",
       " [3, 78, 57],\n",
       " [78, 57, 415],\n",
       " [57, 415, 10],\n",
       " [23, 99, 44],\n",
       " [99, 44, 104],\n",
       " [44, 104, 186],\n",
       " [104, 186, 1047],\n",
       " [186, 1047, 199],\n",
       " [1047, 199, 10],\n",
       " [796, 445, 6407],\n",
       " [445, 6407, 22],\n",
       " [6407, 22, 103],\n",
       " [22, 103, 26],\n",
       " [103, 26, 28],\n",
       " [26, 28, 8477],\n",
       " [28, 8477, 29],\n",
       " [8477, 29, 847],\n",
       " [29, 847, 5667],\n",
       " [847, 5667, 3844],\n",
       " [5667, 3844, 8313],\n",
       " [128, 4, 5118],\n",
       " [4, 5118, 597],\n",
       " [5118, 597, 36],\n",
       " [597, 36, 273],\n",
       " [113, 4, 8873],\n",
       " [4, 8873, 1909],\n",
       " [8873, 1909, 5367],\n",
       " [7, 2, 578],\n",
       " [2, 578, 182],\n",
       " [1205, 3253, 11],\n",
       " [3253, 11, 234],\n",
       " [11, 234, 195],\n",
       " [7, 2, 657],\n",
       " [2, 657, 2953],\n",
       " [657, 2953, 565],\n",
       " [242, 37, 6633],\n",
       " [37, 6633, 11],\n",
       " [6633, 11, 18],\n",
       " [11, 18, 2341],\n",
       " [7664, 2605, 60],\n",
       " [2605, 60, 3650],\n",
       " [60, 3650, 2776],\n",
       " [3650, 2776, 6],\n",
       " [2776, 6, 4],\n",
       " [6, 4, 3650],\n",
       " [4, 3650, 769],\n",
       " [18, 51, 356],\n",
       " [51, 356, 325],\n",
       " [830, 498, 11],\n",
       " [498, 11, 821],\n",
       " [11, 821, 1919],\n",
       " [4423, 1867, 12],\n",
       " [1867, 12, 963],\n",
       " [12, 963, 90],\n",
       " [963, 90, 706],\n",
       " [90, 706, 33],\n",
       " [706, 33, 7210],\n",
       " [33, 7210, 3],\n",
       " [7210, 3, 559],\n",
       " [3, 559, 16],\n",
       " [559, 16, 7],\n",
       " [16, 7, 38],\n",
       " [7, 38, 25],\n",
       " [38, 25, 34],\n",
       " [25, 34, 157],\n",
       " [34, 157, 2],\n",
       " [157, 2, 1767],\n",
       " [2, 1767, 240],\n",
       " [1767, 240, 16],\n",
       " [2196, 5, 2724],\n",
       " [5, 2724, 1249],\n",
       " [2724, 1249, 5612],\n",
       " [1249, 5612, 5],\n",
       " [5612, 5, 3796],\n",
       " [5, 3796, 12],\n",
       " [3796, 12, 7423],\n",
       " [12, 7423, 12],\n",
       " [7423, 12, 6],\n",
       " [12, 6, 7424],\n",
       " [20, 21, 202],\n",
       " [21, 202, 18],\n",
       " [202, 18, 390],\n",
       " [18, 390, 870],\n",
       " [390, 870, 2012],\n",
       " [870, 2012, 4],\n",
       " [2012, 4, 295],\n",
       " [4, 295, 485],\n",
       " [295, 485, 5],\n",
       " [485, 5, 780],\n",
       " [5, 780, 234],\n",
       " [780, 234, 1044],\n",
       " [234, 1044, 10],\n",
       " [1044, 10, 88],\n",
       " [20, 21, 6548],\n",
       " [21, 6548, 24],\n",
       " [6548, 24, 661],\n",
       " [24, 661, 59],\n",
       " [661, 59, 661],\n",
       " [59, 661, 24],\n",
       " [661, 24, 1479],\n",
       " [24, 1479, 10],\n",
       " [1479, 10, 3],\n",
       " [10, 3, 2258],\n",
       " [3, 2258, 6],\n",
       " [2258, 6, 1939],\n",
       " [6, 1939, 5],\n",
       " [1939, 5, 735],\n",
       " [5, 735, 6687],\n",
       " [40, 11, 130],\n",
       " [11, 130, 176],\n",
       " [130, 176, 4],\n",
       " [176, 4, 3799],\n",
       " [4, 3799, 142],\n",
       " [3799, 142, 37],\n",
       " [142, 37, 259],\n",
       " [8939, 15, 27],\n",
       " [15, 27, 195],\n",
       " [27, 195, 12],\n",
       " [195, 12, 42],\n",
       " [12, 42, 4],\n",
       " [42, 4, 51],\n",
       " [4, 51, 16],\n",
       " [51, 16, 587],\n",
       " [16, 587, 17],\n",
       " [587, 17, 4],\n",
       " [17, 4, 40],\n",
       " [4, 40, 91],\n",
       " [40, 91, 131],\n",
       " [370, 8, 40],\n",
       " [8, 40, 286],\n",
       " [40, 286, 1249],\n",
       " [1022, 3299, 1081],\n",
       " [3299, 1081, 1343],\n",
       " [23, 280, 15],\n",
       " [280, 15, 9],\n",
       " [15, 9, 233],\n",
       " [9, 233, 763],\n",
       " [233, 763, 10],\n",
       " [738, 45, 1973],\n",
       " [45, 1973, 7],\n",
       " [1973, 7, 49],\n",
       " [7, 49, 5225],\n",
       " [49, 5225, 2781],\n",
       " [5225, 2781, 14],\n",
       " [2781, 14, 37],\n",
       " [7, 22, 103],\n",
       " [22, 103, 2],\n",
       " [103, 2, 30],\n",
       " [2, 30, 56],\n",
       " [30, 56, 342],\n",
       " [56, 342, 57],\n",
       " [342, 57, 97],\n",
       " [57, 97, 5],\n",
       " [97, 5, 779],\n",
       " [5, 779, 3596],\n",
       " [779, 3596, 1130],\n",
       " [3596, 1130, 234],\n",
       " [1130, 234, 126],\n",
       " [234, 126, 1312],\n",
       " [2274, 701, 790],\n",
       " [701, 790, 12],\n",
       " [790, 12, 591],\n",
       " [12, 591, 12],\n",
       " [591, 12, 6],\n",
       " [12, 6, 514],\n",
       " [6, 514, 8],\n",
       " [514, 8, 40],\n",
       " [113, 5787, 5866],\n",
       " [5787, 5866, 11],\n",
       " [5866, 11, 83],\n",
       " [11, 83, 37],\n",
       " [83, 37, 547],\n",
       " [177, 4, 4851],\n",
       " [4, 4851, 37],\n",
       " [4851, 37, 421],\n",
       " [60, 95, 11],\n",
       " [95, 11, 295],\n",
       " [11, 295, 40],\n",
       " [295, 40, 246],\n",
       " [33, 2400, 7305],\n",
       " [2400, 7305, 39],\n",
       " [7305, 39, 348],\n",
       " [39, 348, 1732],\n",
       " [348, 1732, 10],\n",
       " [22, 158, 3447],\n",
       " [158, 3447, 316],\n",
       " [3447, 316, 31],\n",
       " [316, 31, 115],\n",
       " [31, 115, 16],\n",
       " [115, 16, 151],\n",
       " [16, 151, 23],\n",
       " [151, 23, 22],\n",
       " [23, 22, 103],\n",
       " [22, 103, 16],\n",
       " [947, 17, 290],\n",
       " [17, 290, 15],\n",
       " [290, 15, 3],\n",
       " [15, 3, 118],\n",
       " [3, 118, 4767],\n",
       " [118, 4767, 571],\n",
       " [4767, 571, 11],\n",
       " [571, 11, 1416],\n",
       " [11, 1416, 9],\n",
       " [1416, 9, 2451],\n",
       " [9, 2451, 106],\n",
       " [7, 2, 3361],\n",
       " [2, 3361, 56],\n",
       " [3361, 56, 26],\n",
       " [56, 26, 3],\n",
       " [26, 3, 314],\n",
       " [3, 314, 13],\n",
       " [314, 13, 223],\n",
       " [300, 6774, 14],\n",
       " [6774, 14, 6101],\n",
       " [14, 6101, 1288],\n",
       " [8143, 4, 1693],\n",
       " [4, 1693, 248],\n",
       " [1693, 248, 8],\n",
       " [248, 8, 27],\n",
       " [8, 27, 821],\n",
       " [27, 821, 119],\n",
       " [293, 55, 27],\n",
       " [55, 27, 5226],\n",
       " [27, 5226, 17],\n",
       " [5226, 17, 8661],\n",
       " [1351, 4, 5780],\n",
       " [4, 5780, 378],\n",
       " [63, 1806, 1802],\n",
       " [1806, 1802, 2448],\n",
       " [1802, 2448, 2],\n",
       " [2448, 2, 2288],\n",
       " [2, 2288, 16],\n",
       " [2288, 16, 151],\n",
       " [16, 151, 19],\n",
       " [151, 19, 16],\n",
       " [833, 14, 276],\n",
       " [14, 276, 2],\n",
       " [276, 2, 219],\n",
       " [2, 219, 8],\n",
       " [219, 8, 300],\n",
       " [2502, 8, 307],\n",
       " [70, 8421, 532],\n",
       " [8421, 532, 61],\n",
       " [532, 61, 4],\n",
       " [61, 4, 106],\n",
       " [4, 106, 8],\n",
       " [106, 8, 53],\n",
       " [8, 53, 334],\n",
       " [53, 334, 10],\n",
       " [277, 12, 46],\n",
       " [12, 46, 12],\n",
       " [46, 12, 6],\n",
       " [12, 6, 7],\n",
       " [6, 7, 2],\n",
       " [7, 2, 2359],\n",
       " [2, 2359, 9],\n",
       " [2359, 9, 32],\n",
       " [175, 76, 2],\n",
       " [76, 2, 617],\n",
       " [2, 617, 9],\n",
       " [617, 9, 956],\n",
       " [9, 956, 878],\n",
       " [956, 878, 24],\n",
       " [878, 24, 253],\n",
       " [216, 76, 138],\n",
       " [76, 138, 520],\n",
       " [138, 520, 778],\n",
       " [520, 778, 311],\n",
       " [778, 311, 509],\n",
       " [311, 509, 39],\n",
       " [509, 39, 163],\n",
       " [39, 163, 224],\n",
       " [1353, 241, 4],\n",
       " [241, 4, 3705],\n",
       " [4, 3705, 155],\n",
       " [3705, 155, 144],\n",
       " [155, 144, 140],\n",
       " [146, 4063, 996],\n",
       " [4063, 996, 494],\n",
       " [996, 494, 1066],\n",
       " [494, 1066, 12],\n",
       " [1066, 12, 42],\n",
       " [12, 42, 610],\n",
       " [42, 610, 5355],\n",
       " [3, 28, 1987],\n",
       " [28, 1987, 29],\n",
       " [1987, 29, 58],\n",
       " [29, 58, 5],\n",
       " [58, 5, 1484],\n",
       " [4, 94, 8],\n",
       " [94, 8, 2019],\n",
       " [6908, 411, 14],\n",
       " [411, 14, 4024],\n",
       " [14, 4024, 17],\n",
       " [4024, 17, 7385],\n",
       " [17, 7385, 16],\n",
       " [7385, 16, 507],\n",
       " [16, 507, 5463],\n",
       " [507, 5463, 775],\n",
       " [6833, 186, 120],\n",
       " [186, 120, 1400],\n",
       " [120, 1400, 5],\n",
       " [1400, 5, 1860],\n",
       " [5, 1860, 16],\n",
       " [19, 1762, 139],\n",
       " [1762, 139, 83],\n",
       " [139, 83, 2],\n",
       " [83, 2, 709],\n",
       " [2, 709, 7182],\n",
       " [709, 7182, 6],\n",
       " [7182, 6, 3446],\n",
       " [6, 3446, 168],\n",
       " [3446, 168, 2172],\n",
       " [168, 2172, 11],\n",
       " [2172, 11, 1618],\n",
       " [7684, 8, 1009],\n",
       " [517, 618, 3],\n",
       " [618, 3, 6969],\n",
       " [3, 6969, 2144],\n",
       " [19, 22, 275],\n",
       " [22, 275, 31],\n",
       " [275, 31, 2621],\n",
       " [31, 2621, 526],\n",
       " [2621, 526, 8],\n",
       " [526, 8, 831],\n",
       " [8, 831, 5],\n",
       " [831, 5, 3504],\n",
       " [3, 66, 22],\n",
       " [66, 22, 285],\n",
       " [22, 285, 2907],\n",
       " [285, 2907, 75],\n",
       " [2907, 75, 2181],\n",
       " [75, 2181, 16],\n",
       " [2181, 16, 7855],\n",
       " [2578, 7700, 12],\n",
       " [7700, 12, 695],\n",
       " [12, 695, 7823],\n",
       " [695, 7823, 12],\n",
       " [7823, 12, 6],\n",
       " [12, 6, 598],\n",
       " [6, 598, 5207],\n",
       " [598, 5207, 16],\n",
       " [546, 8119, 15],\n",
       " [8119, 15, 39],\n",
       " [15, 39, 3],\n",
       " [39, 3, 5530],\n",
       " [3, 5530, 2332],\n",
       " [5530, 2332, 5],\n",
       " [2332, 5, 471],\n",
       " [4, 425, 11],\n",
       " [425, 11, 420],\n",
       " [2437, 15, 2181],\n",
       " [15, 2181, 16],\n",
       " [2181, 16, 7013],\n",
       " [16, 7013, 2432],\n",
       " [9, 450, 4967],\n",
       " [450, 4967, 24],\n",
       " [4967, 24, 7],\n",
       " [24, 7, 13],\n",
       " [7, 13, 368],\n",
       " [13, 368, 9],\n",
       " [368, 9, 1645],\n",
       " [22, 285, 3153],\n",
       " [285, 3153, 55],\n",
       " [3153, 55, 4],\n",
       " [55, 4, 8166],\n",
       " [4, 8166, 8091],\n",
       " [8166, 8091, 3161],\n",
       " [371, 2, 3],\n",
       " [2, 3, 559],\n",
       " [3, 559, 14],\n",
       " [559, 14, 6061],\n",
       " [14, 6061, 2470],\n",
       " [19, 2274, 6977],\n",
       " [2274, 6977, 74],\n",
       " [6977, 74, 9],\n",
       " [74, 9, 153],\n",
       " [9, 153, 2],\n",
       " [153, 2, 3],\n",
       " [2, 3, 2242],\n",
       " [3, 2242, 6],\n",
       " [2242, 6, 94],\n",
       " [277, 482, 13],\n",
       " [482, 13, 67],\n",
       " [13, 67, 45],\n",
       " [67, 45, 10],\n",
       " [45, 10, 45],\n",
       " [10, 45, 15],\n",
       " [45, 15, 150],\n",
       " [15, 150, 1292],\n",
       " [150, 1292, 16],\n",
       " [1292, 16, 27],\n",
       " [16, 27, 6942],\n",
       " [27, 6942, 2],\n",
       " [6942, 2, 2488],\n",
       " [2, 2488, 225],\n",
       " [2488, 225, 45],\n",
       " [225, 45, 15],\n",
       " [45, 15, 166],\n",
       " [15, 166, 4],\n",
       " [166, 4, 62],\n",
       " [4, 62, 209],\n",
       " [49, 98, 52],\n",
       " [98, 52, 4],\n",
       " [52, 4, 1285],\n",
       " [6928, 15, 3],\n",
       " [15, 3, 118],\n",
       " [3, 118, 235],\n",
       " [118, 235, 37],\n",
       " [235, 37, 781],\n",
       " [2204, 7, 22],\n",
       " [7, 22, 586],\n",
       " [22, 586, 2811],\n",
       " [586, 2811, 35],\n",
       " [2811, 35, 2],\n",
       " [35, 2, 31],\n",
       " [2, 31, 5668],\n",
       " [20, 21, 2158],\n",
       " [21, 2158, 6617],\n",
       " [2158, 6617, 7],\n",
       " [6617, 7, 34],\n",
       " [7, 34, 225],\n",
       " [34, 225, 157],\n",
       " [225, 157, 10],\n",
       " [157, 10, 88],\n",
       " [1551, 54, 7],\n",
       " [54, 7, 2375],\n",
       " [7, 2375, 8110],\n",
       " [2375, 8110, 8],\n",
       " [8110, 8, 1536],\n",
       " [450, 122, 16],\n",
       " [122, 16, 3148],\n",
       " [231, 14, 3],\n",
       " [14, 3, 3616],\n",
       " [3, 3616, 8],\n",
       " [3616, 8, 31],\n",
       " [8, 31, 1135],\n",
       " [151, 19, 13],\n",
       " [19, 13, 47],\n",
       " [13, 47, 210],\n",
       " [47, 210, 3577],\n",
       " [210, 3577, 3],\n",
       " [3577, 3, 692],\n",
       " [3, 692, 5],\n",
       " [692, 5, 9],\n",
       " [5, 9, 48],\n",
       " [7, 2, 2137],\n",
       " [2, 2137, 9],\n",
       " [2137, 9, 5363],\n",
       " [9, 5363, 2],\n",
       " [5363, 2, 3],\n",
       " [2, 3, 3516],\n",
       " [3, 3516, 1281],\n",
       " [449, 122, 16],\n",
       " [122, 16, 379],\n",
       " [16, 379, 122],\n",
       " [379, 122, 16],\n",
       " [122, 16, 1700],\n",
       " [16, 1700, 17],\n",
       " [1700, 17, 1662],\n",
       " [17, 1662, 98],\n",
       " [1662, 98, 216],\n",
       " [98, 216, 2],\n",
       " [216, 2, 477],\n",
       " [2, 477, 9],\n",
       " [477, 9, 393],\n",
       " [20, 21, 214],\n",
       " [21, 214, 1792],\n",
       " [214, 1792, 8869],\n",
       " [1792, 8869, 427],\n",
       " [8869, 427, 2234],\n",
       " [427, 2234, 2],\n",
       " [2234, 2, 3658],\n",
       " [2, 3658, 455],\n",
       " [3658, 455, 5753],\n",
       " [19, 13, 52],\n",
       " [13, 52, 2],\n",
       " [52, 2, 101],\n",
       " [2, 101, 2197],\n",
       " [101, 2197, 39],\n",
       " [2197, 39, 9],\n",
       " [39, 9, 1689],\n",
       " [7, 2, 463],\n",
       " [2, 463, 57],\n",
       " [463, 57, 484],\n",
       " [20, 21, 6529],\n",
       " [21, 6529, 8],\n",
       " [6529, 8, 9],\n",
       " [8, 9, 527],\n",
       " [9, 527, 6],\n",
       " [527, 6, 4309],\n",
       " [6, 4309, 26],\n",
       " [4309, 26, 9],\n",
       " [26, 9, 2865],\n",
       " [472, 1059, 122],\n",
       " [1059, 122, 3229],\n",
       " [122, 3229, 1059],\n",
       " [8856, 5248, 15],\n",
       " [5248, 15, 4336],\n",
       " [15, 4336, 6],\n",
       " [4336, 6, 151],\n",
       " [6, 151, 19],\n",
       " [19, 13, 361],\n",
       " [13, 361, 69],\n",
       " [361, 69, 499],\n",
       " [69, 499, 11],\n",
       " [499, 11, 9],\n",
       " [11, 9, 58],\n",
       " [19, 3, 162],\n",
       " [3, 162, 2883],\n",
       " [162, 2883, 339],\n",
       " [2883, 339, 266],\n",
       " [339, 266, 3],\n",
       " [266, 3, 631],\n",
       " [237, 58, 2209],\n",
       " [58, 2209, 201],\n",
       " [2209, 201, 4],\n",
       " [201, 4, 1475],\n",
       " [4, 1475, 8],\n",
       " [1475, 8, 3],\n",
       " [8, 3, 3235],\n",
       " [7, 2, 1720],\n",
       " [2, 1720, 496],\n",
       " [1720, 496, 2],\n",
       " [496, 2, 4],\n",
       " [2, 4, 2902],\n",
       " [4, 2902, 744],\n",
       " [1040, 9, 228],\n",
       " [9, 228, 24],\n",
       " [228, 24, 4164],\n",
       " [60, 986, 2],\n",
       " [986, 2, 8911],\n",
       " [2, 8911, 541],\n",
       " [8911, 541, 353],\n",
       " [541, 353, 894],\n",
       " [3, 932, 11],\n",
       " [932, 11, 4],\n",
       " [11, 4, 62],\n",
       " [4, 62, 1982],\n",
       " [62, 1982, 11],\n",
       " [1982, 11, 18],\n",
       " [11, 18, 366],\n",
       " [27, 124, 2],\n",
       " [124, 2, 43],\n",
       " [2, 43, 2311],\n",
       " [43, 2311, 17],\n",
       " [2311, 17, 64],\n",
       " [17, 64, 1637],\n",
       " [3187, 3, 856],\n",
       " [3, 856, 8357],\n",
       " [7, 2, 388],\n",
       " [2, 388, 8],\n",
       " [388, 8, 3],\n",
       " [8, 3, 2242],\n",
       " [3, 2242, 6],\n",
       " [2242, 6, 50],\n",
       " [6, 50, 9],\n",
       " [50, 9, 66],\n",
       " [9, 66, 1294],\n",
       " [209, 3083, 12],\n",
       " [3083, 12, 146],\n",
       " [12, 146, 2911],\n",
       " [146, 2911, 71],\n",
       " [2267, 24, 6867],\n",
       " [24, 6867, 3808],\n",
       " [6867, 3808, 31],\n",
       " [3808, 31, 136],\n",
       " [31, 136, 24],\n",
       " [136, 24, 4267],\n",
       " [22, 63, 84],\n",
       " [63, 84, 23],\n",
       " [84, 23, 2],\n",
       " [23, 2, 41],\n",
       " [2, 41, 14],\n",
       " [41, 14, 31],\n",
       " [14, 31, 2911],\n",
       " [60, 95, 11],\n",
       " [95, 11, 8100],\n",
       " [11, 8100, 2],\n",
       " [8100, 2, 62],\n",
       " [2, 62, 305],\n",
       " [62, 305, 46],\n",
       " [305, 46, 3562],\n",
       " [3, 1365, 5],\n",
       " [1365, 5, 7495],\n",
       " [5, 7495, 17],\n",
       " [7495, 17, 7],\n",
       " [17, 7, 610],\n",
       " [7, 610, 170],\n",
       " [610, 170, 2029],\n",
       " [170, 2029, 487],\n",
       " [2029, 487, 888],\n",
       " [487, 888, 8],\n",
       " [888, 8, 58],\n",
       " [47, 13, 3460],\n",
       " [13, 3460, 3],\n",
       " [3460, 3, 2580],\n",
       " [3, 2580, 10],\n",
       " [2905, 427, 7],\n",
       " [427, 7, 138],\n",
       " [7, 138, 15],\n",
       " [138, 15, 3],\n",
       " [15, 3, 52],\n",
       " [3, 52, 10],\n",
       " [20, 21, 6418],\n",
       " [21, 6418, 1289],\n",
       " [6418, 1289, 26],\n",
       " [1289, 26, 167],\n",
       " [26, 167, 51],\n",
       " [167, 51, 2],\n",
       " [51, 2, 50],\n",
       " [2, 50, 6],\n",
       " [50, 6, 8131],\n",
       " [6, 8131, 208],\n",
       " [8131, 208, 4714],\n",
       " [7, 2, 30],\n",
       " [2, 30, 1458],\n",
       " [30, 1458, 57],\n",
       " [1458, 57, 279],\n",
       " [137, 246, 57],\n",
       " [246, 57, 4],\n",
       " [57, 4, 8282],\n",
       " [4, 8282, 54],\n",
       " [8282, 54, 1487],\n",
       " [54, 1487, 257],\n",
       " [1487, 257, 233],\n",
       " [20, 21, 3106],\n",
       " [21, 3106, 49],\n",
       " [3106, 49, 852],\n",
       " [49, 852, 120],\n",
       " [852, 120, 2268],\n",
       " [120, 2268, 572],\n",
       " [2268, 572, 14],\n",
       " [572, 14, 1306],\n",
       " [14, 1306, 362],\n",
       " [1306, 362, 10],\n",
       " [362, 10, 88],\n",
       " [7, 2, 67],\n",
       " [2, 67, 3],\n",
       " [67, 3, 8346],\n",
       " [3, 8346, 100],\n",
       " [8346, 100, 5],\n",
       " [100, 5, 9],\n",
       " [5, 9, 32],\n",
       " [9, 80, 1487],\n",
       " [80, 1487, 1595],\n",
       " [1487, 1595, 125],\n",
       " [1595, 125, 103],\n",
       " [125, 103, 26],\n",
       " [103, 26, 97],\n",
       " [26, 97, 193],\n",
       " [97, 193, 5],\n",
       " [193, 5, 1942],\n",
       " [5, 1942, 32],\n",
       " [663, 33, 4093],\n",
       " [33, 4093, 5],\n",
       " [4093, 5, 421],\n",
       " [1485, 2283, 420],\n",
       " [7, 47, 3],\n",
       " [47, 3, 5184],\n",
       " [3, 5184, 6887],\n",
       " [5184, 6887, 1688],\n",
       " [6887, 1688, 1002],\n",
       " [1688, 1002, 10],\n",
       " [8138, 5, 5520],\n",
       " [303, 9, 2218],\n",
       " [9, 2218, 228],\n",
       " [151, 7, 2],\n",
       " [7, 2, 2763],\n",
       " [2, 2763, 9],\n",
       " [2763, 9, 2299],\n",
       " [9, 2299, 3],\n",
       " [2299, 3, 129],\n",
       " [3, 129, 119],\n",
       " [4, 571, 11],\n",
       " [571, 11, 116],\n",
       " [11, 116, 36],\n",
       " [116, 36, 1864],\n",
       " [36, 1864, 3490],\n",
       " [1864, 3490, 2],\n",
       " [3490, 2, 3576],\n",
       " [2, 3576, 9],\n",
       " [3576, 9, 1198],\n",
       " [143, 125, 24],\n",
       " [125, 24, 1167],\n",
       " [24, 1167, 26],\n",
       " [1167, 26, 3],\n",
       " [26, 3, 5673],\n",
       " [3, 5673, 1342],\n",
       " [5673, 1342, 6943],\n",
       " [19, 5562, 257],\n",
       " [5562, 257, 47],\n",
       " [257, 47, 335],\n",
       " [47, 335, 30],\n",
       " [335, 30, 2981],\n",
       " [19, 4, 648],\n",
       " [4, 648, 72],\n",
       " [648, 72, 15],\n",
       " [72, 15, 158],\n",
       " [15, 158, 89],\n",
       " [158, 89, 58],\n",
       " [41, 13, 447],\n",
       " [13, 447, 61],\n",
       " [447, 61, 27],\n",
       " [61, 27, 844],\n",
       " [27, 844, 925],\n",
       " [844, 925, 10],\n",
       " [7, 2, 185],\n",
       " [2, 185, 385],\n",
       " [185, 385, 26],\n",
       " [385, 26, 516],\n",
       " [26, 516, 2],\n",
       " [516, 2, 352],\n",
       " [2, 352, 27],\n",
       " [352, 27, 513],\n",
       " [28, 3580, 29],\n",
       " [3580, 29, 81],\n",
       " [29, 81, 22],\n",
       " [81, 22, 63],\n",
       " [22, 63, 1381],\n",
       " [63, 1381, 39],\n",
       " [1381, 39, 270],\n",
       " [3, 141, 5],\n",
       " [141, 5, 2390],\n",
       " [5, 2390, 73],\n",
       " [133, 26, 31],\n",
       " [26, 31, 757],\n",
       " [31, 757, 16],\n",
       " [757, 16, 210],\n",
       " [16, 210, 1301],\n",
       " [210, 1301, 9],\n",
       " [1301, 9, 438],\n",
       " [9, 438, 71],\n",
       " [1157, 4281, 7],\n",
       " [4281, 7, 1904],\n",
       " [7, 1904, 314],\n",
       " [1904, 314, 1019],\n",
       " [314, 1019, 192],\n",
       " [23, 99, 9],\n",
       " [99, 9, 6724],\n",
       " [9, 6724, 4585],\n",
       " [6724, 4585, 14],\n",
       " [4585, 14, 9],\n",
       " [14, 9, 65],\n",
       " [9, 65, 10],\n",
       " [13, 345, 127],\n",
       " [345, 127, 2],\n",
       " [127, 2, 1199],\n",
       " [2, 1199, 45],\n",
       " [1199, 45, 4279],\n",
       " [45, 4279, 5512],\n",
       " [4279, 5512, 316],\n",
       " [166, 472, 324],\n",
       " [472, 324, 81],\n",
       " [324, 81, 13],\n",
       " [81, 13, 63],\n",
       " [13, 63, 448],\n",
       " [7448, 1998, 5361],\n",
       " [949, 543, 33],\n",
       " [543, 33, 1473],\n",
       " [33, 1473, 215],\n",
       " [1473, 215, 96],\n",
       " [215, 96, 3545],\n",
       " [96, 3545, 16],\n",
       " [23, 22, 127],\n",
       " [22, 127, 2],\n",
       " [127, 2, 284],\n",
       " [2, 284, 31],\n",
       " [284, 31, 8818],\n",
       " [31, 8818, 708],\n",
       " [19, 339, 49],\n",
       " [339, 49, 1426],\n",
       " [49, 1426, 3],\n",
       " [1426, 3, 4797],\n",
       " [3, 4797, 6],\n",
       " [4797, 6, 1475],\n",
       " [6, 1475, 120],\n",
       " [1475, 120, 1576],\n",
       " [120, 1576, 369],\n",
       " [1576, 369, 75],\n",
       " [369, 75, 369],\n",
       " [75, 369, 10],\n",
       " [15, 289, 3061],\n",
       " [289, 3061, 2],\n",
       " [3061, 2, 9],\n",
       " [2, 9, 1715],\n",
       " [9, 1715, 1542],\n",
       " [1715, 1542, 6],\n",
       " [1542, 6, 887],\n",
       " [6, 887, 45],\n",
       " [887, 45, 18],\n",
       " [45, 18, 2],\n",
       " [18, 2, 3909],\n",
       " [2, 3909, 10],\n",
       " [22, 285, 4],\n",
       " [285, 4, 342],\n",
       " [4, 342, 188],\n",
       " [346, 22, 103],\n",
       " [22, 103, 536],\n",
       " [103, 536, 596],\n",
       " [536, 596, 39],\n",
       " [596, 39, 180],\n",
       " [39, 180, 3],\n",
       " [180, 3, 527],\n",
       " [3, 527, 60],\n",
       " [527, 60, 334],\n",
       " [47, 3, 8848],\n",
       " [3, 8848, 6801],\n",
       " [8848, 6801, 611],\n",
       " [6801, 611, 74],\n",
       " [611, 74, 5023],\n",
       " [74, 5023, 10],\n",
       " [5790, 3, 699],\n",
       " [3, 699, 12],\n",
       " [699, 12, 42],\n",
       " [12, 42, 158],\n",
       " [42, 158, 3],\n",
       " [158, 3, 5010],\n",
       " [19, 733, 174],\n",
       " [733, 174, 682],\n",
       " [174, 682, 13],\n",
       " [682, 13, 6905],\n",
       " [3, 4505, 294],\n",
       " [4505, 294, 1200],\n",
       " [294, 1200, 15],\n",
       " [1200, 15, 1661],\n",
       " [15, 1661, 3389],\n",
       " [1661, 3389, 471],\n",
       " [3389, 471, 474],\n",
       " [3466, 212, 250],\n",
       " [212, 250, 1873],\n",
       " [3, 35, 1803],\n",
       " [35, 1803, 1794],\n",
       " [50, 9, 160],\n",
       " [9, 160, 6333],\n",
       " [160, 6333, 315],\n",
       " [7, 3, 1365],\n",
       " [3, 1365, 5],\n",
       " [1365, 5, 4138],\n",
       " [5, 4138, 501],\n",
       " [208, 2842, 26],\n",
       " [2842, 26, 3],\n",
       " [26, 3, 5973],\n",
       " [3, 5973, 5013],\n",
       " [5973, 5013, 82],\n",
       " [5013, 82, 705],\n",
       " [7, 2, 802],\n",
       " [2, 802, 3],\n",
       " [802, 3, 111],\n",
       " [3, 111, 5707],\n",
       " [70, 13, 7145],\n",
       " [13, 7145, 59],\n",
       " [7145, 59, 70],\n",
       " [59, 70, 13],\n",
       " [70, 13, 406],\n",
       " [13, 406, 74],\n",
       " [406, 74, 10],\n",
       " [42, 719, 858],\n",
       " [719, 858, 71],\n",
       " [858, 71, 17],\n",
       " [71, 17, 53],\n",
       " [17, 53, 76],\n",
       " [53, 76, 2],\n",
       " [76, 2, 770],\n",
       " [2, 770, 858],\n",
       " [770, 858, 3937],\n",
       " [1144, 4, 8905],\n",
       " [4, 8905, 5749],\n",
       " [8905, 5749, 39],\n",
       " [5749, 39, 470],\n",
       " [39, 470, 8],\n",
       " [470, 8, 8637],\n",
       " [2116, 6932, 6],\n",
       " [6932, 6, 6034],\n",
       " [703, 98, 9],\n",
       " [98, 9, 32],\n",
       " [9, 32, 17],\n",
       " [32, 17, 89],\n",
       " [17, 89, 59],\n",
       " [89, 59, 205],\n",
       " [9, 495, 7834],\n",
       " [495, 7834, 33],\n",
       " [7834, 33, 3222],\n",
       " [13, 33, 4],\n",
       " [33, 4, 3675],\n",
       " [4, 3675, 2784],\n",
       " [3675, 2784, 147],\n",
       " [69, 3, 216],\n",
       " [3, 216, 4865],\n",
       " [216, 4865, 5],\n",
       " [4865, 5, 89],\n",
       " [5, 89, 32],\n",
       " [89, 32, 2],\n",
       " [32, 2, 287],\n",
       " [2, 287, 32],\n",
       " [287, 32, 2228],\n",
       " [3, 4583, 141],\n",
       " [4583, 141, 5],\n",
       " [141, 5, 32],\n",
       " [5, 32, 6],\n",
       " [32, 6, 577],\n",
       " [436, 15, 146],\n",
       " [15, 146, 1773],\n",
       " [146, 1773, 372],\n",
       " [1773, 372, 55],\n",
       " [372, 55, 4],\n",
       " [55, 4, 205],\n",
       " [4, 205, 188],\n",
       " [7, 6954, 4838],\n",
       " [6954, 4838, 34],\n",
       " [4838, 34, 1104],\n",
       " [34, 1104, 6],\n",
       " [1104, 6, 1019],\n",
       " [6, 1019, 9],\n",
       " [1019, 9, 136],\n",
       " [7, 2, 68],\n",
       " [2, 68, 3],\n",
       " [68, 3, 188],\n",
       " [3, 188, 718],\n",
       " [188, 718, 335],\n",
       " [718, 335, 2409],\n",
       " [335, 2409, 2],\n",
       " [2409, 2, 223],\n",
       " [23, 2, 68],\n",
       " [2, 68, 46],\n",
       " [68, 46, 13],\n",
       " [46, 13, 63],\n",
       " [13, 63, 384],\n",
       " [63, 384, 96],\n",
       " [384, 96, 32],\n",
       " [23, 278, 476],\n",
       " [278, 476, 15],\n",
       " [476, 15, 583],\n",
       " [15, 583, 2],\n",
       " [583, 2, 284],\n",
       " [2, 284, 13],\n",
       " [7, 380, 26],\n",
       " [380, 26, 4],\n",
       " [26, 4, 3543],\n",
       " [4, 3543, 453],\n",
       " [3543, 453, 2],\n",
       " [453, 2, 4],\n",
       " [2, 4, 1746],\n",
       " [4, 1746, 2381],\n",
       " [1746, 2381, 31],\n",
       " [2381, 31, 32],\n",
       " [278, 476, 201],\n",
       " [476, 201, 161],\n",
       " [201, 161, 28],\n",
       " [161, 28, 6],\n",
       " ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2424"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(vocab)+1\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs=np.asarray(data)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4, 659],\n",
       "       [659,  72],\n",
       "       [ 72,   2],\n",
       "       ...,\n",
       "       [  4,  89],\n",
       "       [ 89,  58],\n",
       "       [ 58, 662]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length=train_inputs.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets=np.asarray(data)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41822,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='float32')[y]\n",
    "\n",
    "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        #simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        #lstm \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=2, bidirectional=False)\n",
    "        \n",
    "        #fully connected layer\n",
    "        self.linear = nn.Linear(hidden_size*seq_length,vocab_size)\n",
    "        self.fun = nn.Softmax()\n",
    "    \n",
    "    def forward(self, input_word):\n",
    "        #input sequence to embeddings\n",
    "        embedded = self.embed(input_word)\n",
    "        \n",
    "        #passing the embedding to lstm model\n",
    "        output, hidden = self.lstm(embedded)\n",
    "        \n",
    "        #reshaping\n",
    "        output=output.view(output.size(0), -1)\n",
    "        \n",
    "        #fully connected layer\n",
    "        output = self.linear(output)\n",
    "        output = self.fun(output)\n",
    "        \n",
    "        return output,hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\notebook.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m=\u001b[39mlstm(vocab_size\u001b[39m=\u001b[39mvocabulary_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocabulary_size' is not defined"
     ]
    }
   ],
   "source": [
    "model=lstm(vocab_size=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm(\n",
       "  (embed): Embedding(9003, 128)\n",
       "  (lstm): LSTM(128, 256, num_layers=2)\n",
       "  (linear): Linear(in_features=512, out_features=9003, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= Adam(model.parameters(), lr=0.07)\n",
    "\n",
    "#loss\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    #set the model to train\n",
    "    model.train()\n",
    "    tr_loss=0    \n",
    "    \n",
    "    #clearing the Gradients \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #predict the output\n",
    "    y_pred, (state_h, state_c) = model(torch.from_numpy(train_inputs))\n",
    "    \n",
    "    #compute the loss\n",
    "    loss=criterion(y_pred,torch.from_numpy(train_targets))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "\n",
    "    #update the parameters\n",
    "    optimizer.step()\n",
    "    tr_loss = loss.item()\n",
    "\n",
    "    print(\"Epoch : \",epoch,\"loss : \",loss)\n",
    "\n",
    "no_epoch=4\n",
    "losses=[]\n",
    "for epoch in range(1,no_epoch+1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41822, 2)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[0].reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4, 659],\n",
       "       [659,  72],\n",
       "       [ 72,   2],\n",
       "       ...,\n",
       "       [  4,  89],\n",
       "       [ 89,  58],\n",
       "       [ 58, 662]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 2]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['how', 'to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "predict_next_word('jk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['—']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(text):\n",
    "    #set the model to evaluation\n",
    "    model.eval()\n",
    "\n",
    "    #converting to array\n",
    "    with torch.no_grad():\n",
    "        #converting to tensor\n",
    "        \n",
    "        #predicting the output\n",
    "        predict,(hidden,cell)=model(torch.from_numpy(np.asarray([7, 2]).reshape(1,-1)))\n",
    "    \n",
    "    #applying the softmax layer\n",
    "    softmax = torch.exp(predict)\n",
    "    prob = list(softmax.numpy())\n",
    "    \n",
    "    #index of the predict word\n",
    "    predictions = np.argmax(prob)\n",
    "\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  4, 659], dtype=torch.int32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(train_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"next word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "predict_next_word(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Files in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[172, 468, 766]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# vocab.lookup_tokens([468, 1585, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning']"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = './comments/'\n",
    "all_headlines = []\n",
    "for filename in os.listdir(curr_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(curr_dir + filename)\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Finding an Expansive View  of a Forgotten People in Niger', 'And Now,  the Dreaded Trump Curse', 'Venezuela’s Descent Into Dictatorship', 'Stain Permeates Basketball Blue Blood', 'Taking Things for Granted', 'The Caged Beast Awakens', 'An Ever-Unfolding Story', 'O’Reilly Thrives as Settlements Add Up', 'Mouse Infestation', 'Divide in G.O.P. Now Threatens Trump Tax Plan']\n"
     ]
    }
   ],
   "source": [
    "all_headlines = [line for line in all_headlines if line!= \"Unknown\"]\n",
    "print(all_headlines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(t for t in txt if t not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finding an expansive view  of a forgotten people in niger', 'and now  the dreaded trump curse', 'venezuelas descent into dictatorship', 'stain permeates basketball blue blood', 'taking things for granted', 'the caged beast awakens', 'an everunfolding story', 'oreilly thrives as settlements add up', 'mouse infestation', 'divide in gop now threatens trump tax plan', 'variety puzzle acrostic', 'they can hit a ball 400 feet but play catch thats tricky', 'in trump country shock at trump budget cuts', 'why is this hate different from all other hate']\n"
     ]
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "print(corpus[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'now', 'the', 'dreaded', 'trump', 'curse']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(corpus), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_of_token(corpus):\n",
    "    input_sequnces = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer(line)\n",
    "        for i in range(1,len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            _n_gram_sequence  = vocab(n_gram_sequence)\n",
    "            input_sequnces.append(_n_gram_sequence)\n",
    "    return input_sequnces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_seq = get_sequence_of_token(corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2423"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = len(vocab)\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='float32')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_torch(arr,maxlen):\n",
    "    seq_len = len(arr)\n",
    "    padding = (maxlen-seq_len,0)\n",
    "    pad = torch.nn.ZeroPad2d(padding)\n",
    "    return pad(torch.tensor(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(inp_seq):\n",
    "    max_sequence_len = max([len(x) for x in inp_seq])\n",
    "    res_1 = [pad_sequences_torch(i,max_sequence_len) for i in inp_seq]\n",
    "    input_sequences = np.array(res_1)\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4806"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, inp_seq):\n",
    "        self.inp_seq = inp_seq\n",
    "        max_sequence_len = max([len(x) for x in self.inp_seq])\n",
    "        res_1 = [pad_sequences_torch(i,max_sequence_len) for i in self.inp_seq]\n",
    "        self.input_sequences = np.array(res_1)\n",
    "         \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        fetures, label = self.input_sequences[index,:-1],self.input_sequences[index,-1]\n",
    "        label = to_categorical(label, num_classes=total_words)\n",
    "        return fetures, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inp_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(inp_seq=inp_seq)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = iter(train_loader)\n",
    "feature, label = next(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,   20, 2397, 1222])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1682e+00,  3.6346e-01, -1.6031e+00, -1.2317e+00, -1.2398e+00,\n",
       "        -2.1165e-01, -9.1523e-01, -1.4533e+00, -3.4596e-01, -1.6536e+00,\n",
       "         8.5156e-01, -1.1065e+00, -7.8382e-01,  6.3386e-01,  9.7227e-01,\n",
       "        -8.7601e-01, -5.6772e-01, -1.0541e+00, -1.4056e-01, -2.8671e-01,\n",
       "        -4.3447e-01, -7.7243e-01, -2.0317e-01, -5.0626e-01,  1.2863e-01,\n",
       "        -1.7923e-01,  3.4295e-01,  5.1880e-01, -1.1758e+00, -1.4124e+00,\n",
       "         4.1626e-01, -1.3540e-01,  2.6110e-01,  3.1680e-01, -2.0731e+00,\n",
       "        -1.3371e-01,  2.3793e-01,  9.4596e-01,  6.5432e-01, -1.0252e-01,\n",
       "        -4.5700e-01,  1.4070e+00, -4.2537e-01,  6.4142e-01, -9.5792e-01,\n",
       "        -7.4139e-01,  7.7897e-01,  4.5210e-01,  1.3707e+00, -5.5293e-01,\n",
       "         8.1858e-01, -3.2324e-01,  1.4070e+00, -1.0768e+00,  1.0443e-03,\n",
       "         8.6933e-01,  7.4814e-01,  1.6104e+00, -8.2147e-01,  7.1884e-01,\n",
       "        -3.2226e-01, -1.6827e-01,  1.3920e-01, -2.0459e+00,  1.0289e+00,\n",
       "        -7.8053e-02,  6.8039e-01,  8.2639e-02, -1.8656e-01,  8.3476e-01,\n",
       "        -3.7094e-01, -1.6818e-01,  2.1816e-01, -3.9125e-02,  1.1551e+00,\n",
       "        -1.2724e+00,  2.8284e-01,  1.1268e+00,  9.6474e-01, -1.0672e+00,\n",
       "         5.0975e-01, -1.7142e+00, -6.8115e-01,  1.7085e-01, -9.7429e-01,\n",
       "        -1.7426e+00,  5.2331e-01, -2.6400e-01, -5.9295e-01,  1.8163e+00,\n",
       "        -5.9211e-02,  4.8075e-01,  6.5837e-01,  8.4176e-01,  9.6424e-01,\n",
       "         6.2302e-01,  3.2440e-01,  2.4135e-02, -2.6791e-02, -1.7977e-01,\n",
       "        -1.8736e+00,  2.3024e-01,  7.3915e-01, -9.0146e-01, -1.4880e-02,\n",
       "         1.7964e-01, -1.6343e+00,  7.8327e-01,  3.9075e-02,  1.3479e-01,\n",
       "        -1.1063e+00, -1.0485e-01,  4.2541e-01,  5.8359e-01, -1.4485e+00,\n",
       "        -4.8231e-01,  1.4960e+00, -1.5423e-01, -2.2229e-01, -2.3485e+00,\n",
       "         4.5723e-02, -8.4587e-01,  1.5651e+00, -7.2005e-01, -1.0701e+00,\n",
       "        -1.1233e+00,  7.2800e-01,  1.2580e+00,  5.4744e-01,  1.4652e-01,\n",
       "         1.5325e+00, -2.2159e+00, -2.1563e+00,  1.2189e+00, -7.9056e-01,\n",
       "         7.9074e-01,  4.7393e-01, -1.2601e+00, -1.9375e-01, -4.0949e-01,\n",
       "        -4.4264e-01,  7.4034e-01,  6.8447e-01, -6.5521e-01,  6.3646e-01,\n",
       "         6.9233e-01,  4.4202e-01,  1.4175e+00, -3.6308e-02, -5.5835e-01,\n",
       "         1.5328e-01,  4.2376e-02,  1.7864e+00,  1.5204e+00,  1.0386e+00,\n",
       "        -5.5902e-01,  1.1488e+00, -2.2734e-02,  1.7809e+00,  2.0742e-01,\n",
       "         1.0182e+00, -2.8717e+00,  1.7617e+00,  2.3716e-01, -6.8713e-02,\n",
       "        -5.1861e-01,  2.6250e-01,  5.6905e-01, -3.2246e-01,  2.7731e-01,\n",
       "         5.2886e-01,  1.5696e-01, -5.5520e-02, -3.8086e-01,  7.3281e-01,\n",
       "         9.6474e-01, -9.7051e-01, -8.7347e-01, -1.3477e+00, -1.5079e-01,\n",
       "         3.7671e-01,  2.5727e-01, -1.5530e+00, -1.8564e+00, -2.3763e-01,\n",
       "         7.7046e-01,  5.5598e-01, -2.0081e+00, -1.0095e+00, -2.7791e-01,\n",
       "         5.4994e-01,  1.0495e+00, -9.9204e-01,  6.1075e-01,  2.1037e-01,\n",
       "        -1.3024e+00, -1.5926e+00, -2.7777e-01, -1.3170e-01,  3.1163e-03],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed(feature[0])\n",
    "embed(feature[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 200])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(torch.tensor(vocab(tokenizer(corpus[1])))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, label, max_seq = generate_padded_sequences(inp_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,  185],\n",
       "       [   0,    0,    0, ...,    0,  185,   18],\n",
       "       [   0,    0,    0, ...,  185,   18, 1219],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  101, 2180,   57],\n",
       "       [   0,    0,    0, ..., 2180,   57,  347],\n",
       "       [   0,    0,    0, ...,   57,  347,   95]], dtype=int64)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4806, 2423)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2423, 10)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Embedding(total_words, 10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[185, 18]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=10, hidden_size=100):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        #lstm \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=1, bidirectional=False)\n",
    "        \n",
    "        #fully connected layer\n",
    "        self.linear = nn.Linear(hidden_size*max_seq,vocab_size)\n",
    "        self.fun = nn.Softmax()\n",
    "    \n",
    "    def forward(self, input_word):\n",
    "        #input sequence to embeddings\n",
    "        embedded = self.embed(input_word)\n",
    "        \n",
    "        #passing the embedding to lstm model\n",
    "        output, hidden = self.lstm(embedded)\n",
    "        \n",
    "        #reshaping\n",
    "        output=output.view(output.size(0), -1)\n",
    "        \n",
    "        #fully connected layer\n",
    "        output = self.linear(output)\n",
    "        output = self.fun(output)\n",
    "        \n",
    "        return output,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "      \n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "     \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x=x.long()\n",
    "        \n",
    "        # embedding and lstm_out \n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack up lstm layers\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout, fc layer and final sigmoid layer\n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        # reshaping out layer to batch_size * seq_length * output_size\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        print(out.size())\n",
    "        # return last batch\n",
    "        out = out[:, -1]\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weights = next(self.parameters()).data\n",
    "        if(train_on_gpu):\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), \n",
    "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "   \n",
    "    h = tuple([each.data for each in hidden])\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "  \n",
    "    inputs, targets = inp.to(device), target.to(device)\n",
    "    \n",
    "    output, h = rnn(inputs, h)\n",
    "    \n",
    "    loss = criterion(output, targets)\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "            break\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 18  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 1\n",
    "\n",
    "# data loader - do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab)\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 200\n",
    "# Hidden Dimension\n",
    "hidden_dim = 250\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Padding_idx must be within num_embeddings",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\notebook.ipynb Cell 92\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y204sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rnn \u001b[39m=\u001b[39m RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y204sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(rnn)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y204sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m train_on_gpu:\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\notebook.ipynb Cell 92\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y204sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y204sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39msuper\u001b[39m(RNN, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y204sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(\u001b[39m18\u001b[39;49m,vocab_size, embedding_dim)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y204sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLSTM(embedding_dim, hidden_dim, n_layers, dropout\u001b[39m=\u001b[39mdropout, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y204sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m vocab_size\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:133\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mif\u001b[39;00m padding_idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m padding_idx \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[39massert\u001b[39;00m padding_idx \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_embeddings, \u001b[39m'\u001b[39m\u001b[39mPadding_idx must be within num_embeddings\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    134\u001b[0m     \u001b[39melif\u001b[39;00m padding_idx \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    135\u001b[0m         \u001b[39massert\u001b[39;00m padding_idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_embeddings, \u001b[39m'\u001b[39m\u001b[39mPadding_idx must be within num_embeddings\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Padding_idx must be within num_embeddings"
     ]
    }
   ],
   "source": [
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "print(rnn)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = rnn.init_hidden(batch_size)\n",
    "h = tuple([each.data for each in hidden])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  346, 2153])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = tokenizer('united states')\n",
    "text  = vocab(token_list)\n",
    "text = pad_sequences_torch(text,18)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rnn(text.reshape(1,-1),h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(156)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output[-1][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['money']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([156])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(label, input):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    model.hidden = model.init_hidden()\n",
    "    print(input)\n",
    "    output = model(input)[-1]\n",
    "    loss = criterion(output.unsqueeze(0), label)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    return output.unsqueeze(0), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 1\n",
    "learning_rate = 0.04 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0, 213,  59]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 27, 20,  3]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2375]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1193, 2256,   66, 1357]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 128,  91,   8,   6,\n",
      "          27, 206, 135,  34]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,  12, 815]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,   43, 1111,   27,  120,  516,   66,\n",
      "          525,  129,    7,  112,   66,  695]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0, 106, 529]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    2, 1912,  351,    2]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    2, 1904,  208,    5]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   6,   2, 590]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2401,\n",
      "          780,   11,    4,  281,  196,    8]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,  25, 618,   8]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0, 282,  21, 106]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1196,\n",
      "           29,  399, 1533, 2067, 2055,  107]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   2, 136,   5]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2, 391,\n",
      "           2, 363,   7,   2]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  577,    6, 1851]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1586,  743, 1979]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,  11, 242]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1067]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1933,  782,    3]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1496,    7]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    6, 2303]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   61,   16, 2060]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  14,\n",
      "          58, 252,  11,  83]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  357,   32, 1524,    8,    2]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 149]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 61]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           43,   10,   11,  467, 1486,    6]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   54,\n",
      "            9,  637,    5, 1829,  843, 2199]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    2, 1904]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1661, 2041,\n",
      "         1718,    9,  113,  236,  972,   68]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,  37, 133,   2]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    2, 2330,    5, 1199]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    3,  537, 2345,    6]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  178,  639,\n",
      "          519,    6, 1733, 1907,  762,    8]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1244, 2170, 2360]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   3, 889]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12, 31, 33]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 829]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1900,   20,\n",
      "          179,   46,  328,  137, 1804,   23]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  14, 143,\n",
      "           3, 313, 246,   5]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    2, 1679,    5]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1369, 2347]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 16, 53]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  202,\n",
      "          217, 2255, 1098,    8,    3, 1525]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            2, 1263,   32,   57,  506,    4]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  26, 198, 332,\n",
      "           3, 602, 773,   4]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  178,  639,  519,    6, 1733]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1685,  397,  327]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,   42,    4, 2228, 1066,   52]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0, 618,  40,  11]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,  102,   46, 1419,    3,\n",
      "          800,  688, 1252,   28, 1831,  915]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 49]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2375,  474]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12, 31]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,   11,  242, 2386, 1308]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           2, 317,   5, 560]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  20,   4, 326,\n",
      "          37,   4,  11,   7]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 140]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,   41,  600,  655,    4,  406,\n",
      "           41,  436,    3, 1149,    9, 1240]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  275,    4,    2, 1577]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    6,  321,    5,   12, 1633]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  601, 2276,    6,    2]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22, 94]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,   54,    9, 2026, 1251]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1820]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  20,   4,\n",
      "          15, 195,  42,  22]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            2,   92, 1654,    6,    2,   92]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1068,  320, 2353]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,   60, 2192,    3, 2018]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 198]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1626, 1185, 1189, 1249]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0, 128,  91,   8]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  358, 1977]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   2, 895]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 327,\n",
      "          10, 238,   7, 238]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,   25,  332, 2066, 2270,    8]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  491,  494,   13, 1902,  799]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   36, 2177,  817]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1988]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1018,  196,    8]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,  13,   2,  86,   5, 119,\n",
      "           3, 354,   4,  90]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           3,  12, 572, 559]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2189,   37,   42]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1725]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    2, 1791]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    2, 1156]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1225,  378,  122]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 266, 174,   7,\n",
      "         284,  24,  56, 125]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,  14, 359]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,  431, 2038,  597]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0, 121,  93]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  16, 558,   4,\n",
      "         267,  63, 422,  10]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    2, 1335,   13,    2]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1691,  495,  564]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    2, 2016]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 43]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,  421,  300, 2043,  188,   11]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1036]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,   54,    9,  637,    5,\n",
      "         1829,  843, 2199,   37,   40, 1053]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1067, 2124]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 43]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1432,  296,  234]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   14,\n",
      "            3, 1675,   17,  789, 1629,    4]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         513, 963, 177, 561]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,   98, 1882,    9,  801]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,  14, 360]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0, 886, 665]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 366]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 39,  2]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  69,  84, 319,\n",
      "          21,  34,  24,  44]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    9,   11,    3,  186,\n",
      "            8,   25,  496,    7,    3, 1124]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,   39,   60,  287,   10,\n",
      "           14,   60, 1923,    6,    2,  261]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    2, 1825,  242,  326]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   2, 331]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1019]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  901,  507,\n",
      "         2328,    3,   83,    4, 1557, 1932]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  168,\n",
      "          226,   34, 1894, 2051,  792,   10]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1221,    2,   36]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          41, 600, 655,   4]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2422,   96]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           12,  365,   87, 2304,  710, 1864]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,   85, 1712,   10]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   2, 114]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2329]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2, 86,  5,  3]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1084,\n",
      "         1983, 2090, 2336,  219,    3,  268]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2254,  181,  149]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0, 1100,    5,   25,  409,\n",
      "         1619,    3, 1532,    5,   69, 1746]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0, 357,  32]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 43, 10]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1626]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          69,  84, 319,  21]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0, 275,   4]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   23,  577,\n",
      "         2034, 1194, 2077, 2212, 2082,   14]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          807, 1614,   14, 1380,  192,  551]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0, 932,   5, 456]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           78,  592, 1414,   13,   38,   73]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1221,\n",
      "            2,   36, 1797,   32,   11,   10]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 2370,  262,  123,   44]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,   54,    9, 2265,  456,  285]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 264,\n",
      "         146, 445, 157, 134]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0, 139,  21]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 125,\n",
      "           3, 109, 231,   4]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  3]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  4]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 345]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14,  2]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 59,  5]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 1530, 2069,  573,   19]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   81,\n",
      "            6, 1779,  410,    3,  590,    8]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1283,   94,   47]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           2, 331,  85,  23]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   12,\n",
      "          365,   87, 2304,  710, 1864, 1895]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 367,  68, 170,\n",
      "          44,  34, 370,  56]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 27]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1649]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 1016, 1346]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  43,\n",
      "          10,  27, 116, 109]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1137]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,  45, 221]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0, 1244, 2170, 2360,    9,  347,\n",
      "            5,  975,    7,  385,  181,    6]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,  198, 2188,  830,    6]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 2146, 1807,  395]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1613, 1393,    3,  120,    5]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  251,\n",
      "            6,  276,   99, 2249,   11,   63]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 261]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    2, 1637,\n",
      "          164,    5, 2333,  631,   14,   11]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,  117, 1553]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,  42, 359]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 1130]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,   54,    9,  154,   40,\n",
      "            2,  146, 2029,  152,  112,  358]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          12, 495,   6,   3]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           19,  564,    4, 1082,    7,  222]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, 2300,  680]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,   12, 1931, 2332,  158,  526]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         1783,   23,    3, 2287,  490,  652]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 139]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    2,  114, 2226]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 591, 414, 325,\n",
      "         199,  55, 548,   2]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, 2406]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,   96,    4, 1543, 2339]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, 1133, 1972,  979,  257, 1742]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, 1361,  611,   23]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,  12, 528]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 43]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          10, 221, 161,   3]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,   58, 1869]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,   14,   58,  252,   11,\n",
      "           83,  159,    7, 1991,  719,   47]])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 828,   3,\n",
      "          87, 216, 811, 903]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,   61, 1052,   87,   70]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    3, 2168,    6,    2,  405]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\notebook.ipynb Cell 90\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y224sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y224sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     current_loss  \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y224sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, (features, labels) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(train_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y224sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         features \u001b[39m=\u001b[39;49m features\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y224sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         labels \u001b[39m=\u001b[39;49m labels\u001b[39m.\u001b[39;49mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m samples \u001b[39min\u001b[39;49;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39;49mas_tensor(b) \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    current_loss  = 0\n",
    "\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "       \n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        out, loss = train(labels,features)\n",
    "\n",
    "    current_loss += loss\n",
    "    \n",
    "    print(f'Epoch: {epoch} , Loss {current_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_torch(arr,maxlen):\n",
    "    seq_len = len(arr)\n",
    "    padding = (maxlen-seq_len,0)\n",
    "    pad = torch.nn.ZeroPad2d(padding)\n",
    "    return pad(torch.tensor(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0, 2341, 1104,  148])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = tokenizer('venezuelas descent into')\n",
    "text  = vocab(token_list)\n",
    "text = pad_sequences_torch(text,18)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\Course\\Next-Word-Prediction\\notebook.ipynb Cell 103\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y162sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mhidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit_hidden()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Course/Next-Word-Prediction/notebook.ipynb#Y162sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output \u001b[39m=\u001b[39m model(text\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.hidden = model.init_hidden()\n",
    "output = model(text.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the']"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
